{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Science-Qualifications\" data-toc-modified-id=\"Data-Science-Qualifications-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Science Qualifications</a></span><ul class=\"toc-item\"><li><span><a href=\"#AWS-Certified-Machine-Learning-Specialty\" data-toc-modified-id=\"AWS-Certified-Machine-Learning-Specialty-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>AWS Certified Machine Learning Specialty</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-Links\" data-toc-modified-id=\"Useful-Links-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Useful Links</a></span><ul class=\"toc-item\"><li><span><a href=\"#Official-exam-page-and-AWS-prep-material\" data-toc-modified-id=\"Official-exam-page-and-AWS-prep-material-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>Official exam page and AWS prep material<br></a></span></li><li><span><a href=\"#Linux-Academy-gives-you-access-to-an-AWS-console-to-participate-in-interacgtive-labs\" data-toc-modified-id=\"Linux-Academy-gives-you-access-to-an-AWS-console-to-participate-in-interacgtive-labs-1.1.1.2\"><span class=\"toc-item-num\">1.1.1.2&nbsp;&nbsp;</span>Linux Academy gives you access to an AWS console to participate in interacgtive labs<br></a></span></li><li><span><a href=\"#A-cloud-guru-has-an-excellent-exam-simulator-to-practice-exam-type-questions\" data-toc-modified-id=\"A-cloud-guru-has-an-excellent-exam-simulator-to-practice-exam-type-questions-1.1.1.3\"><span class=\"toc-item-num\">1.1.1.3&nbsp;&nbsp;</span>A cloud guru has an excellent exam simulator to practice exam type questions<br></a></span></li><li><span><a href=\"#Useful-blog-on-exam-preparation\" data-toc-modified-id=\"Useful-blog-on-exam-preparation-1.1.1.4\"><span class=\"toc-item-num\">1.1.1.4&nbsp;&nbsp;</span>Useful blog on exam preparation<br></a></span></li><li><span><a href=\"#crash-course-from-o'reilly-online-learning-along-with-associated-links-and-material\" data-toc-modified-id=\"crash-course-from-o'reilly-online-learning-along-with-associated-links-and-material-1.1.1.5\"><span class=\"toc-item-num\">1.1.1.5&nbsp;&nbsp;</span>crash course from o'reilly online learning along with associated links and material<br></a></span></li></ul></li></ul></li><li><span><a href=\"#CRT020-Databricks-Certified-Associate-Developer-for-Apache-Spark-2.4-with-Scala-2.11\" data-toc-modified-id=\"CRT020-Databricks-Certified-Associate-Developer-for-Apache-Spark-2.4-with-Scala-2.11-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>CRT020 Databricks Certified Associate Developer for Apache Spark 2.4 with Scala 2.11</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-Links\" data-toc-modified-id=\"Useful-Links-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Useful Links</a></span><ul class=\"toc-item\"><li><span><a href=\"#Official-exam-page\" data-toc-modified-id=\"Official-exam-page-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>Official exam page<br></a></span></li><li><span><a href=\"#Exam-booking-details\" data-toc-modified-id=\"Exam-booking-details-1.2.1.2\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;</span>Exam booking details<br></a></span></li><li><span><a href=\"#Study-guide\" data-toc-modified-id=\"Study-guide-1.2.1.3\"><span class=\"toc-item-num\">1.2.1.3&nbsp;&nbsp;</span>Study guide<br></a></span></li><li><span><a href=\"#Blog-on-linkedin-with-detailed-prep-notes\" data-toc-modified-id=\"Blog-on-linkedin-with-detailed-prep-notes-1.2.1.4\"><span class=\"toc-item-num\">1.2.1.4&nbsp;&nbsp;</span>Blog on linkedin with detailed prep notes<br></a></span></li><li><span><a href=\"#Useful-FAQ-answers-for-exam\" data-toc-modified-id=\"Useful-FAQ-answers-for-exam-1.2.1.5\"><span class=\"toc-item-num\">1.2.1.5&nbsp;&nbsp;</span>Useful FAQ answers for exam</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Linux\" data-toc-modified-id=\"Linux-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Linux</a></span><ul class=\"toc-item\"><li><span><a href=\"#Virtual-Box\" data-toc-modified-id=\"Virtual-Box-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Virtual Box</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-vboxmanage-to-configure-virtualbox\" data-toc-modified-id=\"Using-vboxmanage-to-configure-virtualbox-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Using vboxmanage to configure virtualbox<br></a></span></li><li><span><a href=\"#Increase-size-of-VM-partition\" data-toc-modified-id=\"Increase-size-of-VM-partition-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Increase size of VM partition</a></span></li></ul></li><li><span><a href=\"#Setup-PATH-variable-correctly\" data-toc-modified-id=\"Setup-PATH-variable-correctly-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Setup PATH variable correctly<br></a></span></li><li><span><a href=\"#Immediate-activation-of-any-path-updates-without-closing-session\" data-toc-modified-id=\"Immediate-activation-of-any-path-updates-without-closing-session-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Immediate activation of any path updates without closing session<br></a></span></li><li><span><a href=\"#Symbolic-links\" data-toc-modified-id=\"Symbolic-links-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Symbolic links</a></span></li><li><span><a href=\"#Linux-Server\" data-toc-modified-id=\"Linux-Server-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Linux Server</a></span></li><li><span><a href=\"#SSH-Notes\" data-toc-modified-id=\"SSH-Notes-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>SSH Notes</a></span></li><li><span><a href=\"#Common-Bugs\" data-toc-modified-id=\"Common-Bugs-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Common Bugs</a></span><ul class=\"toc-item\"><li><span><a href=\"#write-on-sudo-owned-file-when-did’nt-use-sudo-to-open\" data-toc-modified-id=\"write-on-sudo-owned-file-when-did’nt-use-sudo-to-open-2.7.1\"><span class=\"toc-item-num\">2.7.1&nbsp;&nbsp;</span>write on sudo owned file when did’nt use sudo to open<br></a></span></li><li><span><a href=\"#Colour-scheme\" data-toc-modified-id=\"Colour-scheme-2.7.2\"><span class=\"toc-item-num\">2.7.2&nbsp;&nbsp;</span>Colour scheme<br></a></span></li><li><span><a href=\"#Apt-get-stalling-on-waiting-for-headers\" data-toc-modified-id=\"Apt-get-stalling-on-waiting-for-headers-2.7.3\"><span class=\"toc-item-num\">2.7.3&nbsp;&nbsp;</span>Apt-get stalling on waiting for headers<br></a></span></li><li><span><a href=\"#Problems-with-Linux-software-updater\" data-toc-modified-id=\"Problems-with-Linux-software-updater-2.7.4\"><span class=\"toc-item-num\">2.7.4&nbsp;&nbsp;</span>Problems with Linux software updater<br></a></span></li></ul></li><li><span><a href=\"#Docker\" data-toc-modified-id=\"Docker-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Docker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Install-docker\" data-toc-modified-id=\"Install-docker-2.8.1\"><span class=\"toc-item-num\">2.8.1&nbsp;&nbsp;</span>Install docker</a></span></li><li><span><a href=\"#Docker-commands\" data-toc-modified-id=\"Docker-commands-2.8.2\"><span class=\"toc-item-num\">2.8.2&nbsp;&nbsp;</span>Docker commands</a></span></li><li><span><a href=\"#Docker-Compose-commands\" data-toc-modified-id=\"Docker-Compose-commands-2.8.3\"><span class=\"toc-item-num\">2.8.3&nbsp;&nbsp;</span>Docker-Compose commands</a></span></li></ul></li></ul></li><li><span><a href=\"#Python\" data-toc-modified-id=\"Python-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-links\" data-toc-modified-id=\"Useful-links-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Useful links</a></span></li><li><span><a href=\"#Virtual-Environments\" data-toc-modified-id=\"Virtual-Environments-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Virtual Environments</a></span><ul class=\"toc-item\"><li><span><a href=\"#Install-virtualenv-and-virtualenvwrapper\" data-toc-modified-id=\"Install-virtualenv-and-virtualenvwrapper-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Install virtualenv and virtualenvwrapper<br></a></span></li><li><span><a href=\"#Wrapper-syntax\" data-toc-modified-id=\"Wrapper-syntax-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Wrapper syntax<br></a></span></li><li><span><a href=\"#Virtualenv-syntax\" data-toc-modified-id=\"Virtualenv-syntax-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Virtualenv syntax<br></a></span></li><li><span><a href=\"#Conda-syntax\" data-toc-modified-id=\"Conda-syntax-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Conda syntax</a></span></li><li><span><a href=\"#Pip-setup\" data-toc-modified-id=\"Pip-setup-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span>Pip setup<br></a></span></li></ul></li><li><span><a href=\"#IDE-Tips\" data-toc-modified-id=\"IDE-Tips-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>IDE Tips</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vim\" data-toc-modified-id=\"Vim-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Vim</a></span></li><li><span><a href=\"#Jupyter\" data-toc-modified-id=\"Jupyter-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Jupyter</a></span></li><li><span><a href=\"#Jupyter-server\" data-toc-modified-id=\"Jupyter-server-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Jupyter server</a></span></li><li><span><a href=\"#Pycharm\" data-toc-modified-id=\"Pycharm-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>Pycharm</a></span></li></ul></li><li><span><a href=\"#Data-Collection\" data-toc-modified-id=\"Data-Collection-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Data Collection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Web-Scraping/Crawling\" data-toc-modified-id=\"Web-Scraping/Crawling-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Web Scraping/Crawling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic-xpath-syntax\" data-toc-modified-id=\"Basic-xpath-syntax-3.4.1.1\"><span class=\"toc-item-num\">3.4.1.1&nbsp;&nbsp;</span>Basic xpath syntax</a></span></li><li><span><a href=\"#Lxml-Approach-(see-example-scripts)\" data-toc-modified-id=\"Lxml-Approach-(see-example-scripts)-3.4.1.2\"><span class=\"toc-item-num\">3.4.1.2&nbsp;&nbsp;</span>Lxml Approach (see example scripts)</a></span></li><li><span><a href=\"#Scrapy\" data-toc-modified-id=\"Scrapy-3.4.1.3\"><span class=\"toc-item-num\">3.4.1.3&nbsp;&nbsp;</span>Scrapy</a></span></li><li><span><a href=\"#Create-new-project\" data-toc-modified-id=\"Create-new-project-3.4.1.4\"><span class=\"toc-item-num\">3.4.1.4&nbsp;&nbsp;</span>Create new project</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-formatting/Wrangling\" data-toc-modified-id=\"Data-formatting/Wrangling-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Data formatting/Wrangling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Regular-Expressions\" data-toc-modified-id=\"Regular-Expressions-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Regular Expressions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-Links\" data-toc-modified-id=\"Useful-Links-3.5.1.1\"><span class=\"toc-item-num\">3.5.1.1&nbsp;&nbsp;</span>Useful Links</a></span></li></ul></li><li><span><a href=\"#String-formatting\" data-toc-modified-id=\"String-formatting-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>String formatting</a></span></li><li><span><a href=\"#Pandas-data-formatting\" data-toc-modified-id=\"Pandas-data-formatting-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>Pandas data formatting</a></span><ul class=\"toc-item\"><li><span><a href=\"#prevent-Setting-WithCopyWarning-in-pandas\" data-toc-modified-id=\"prevent-Setting-WithCopyWarning-in-pandas-3.5.3.1\"><span class=\"toc-item-num\">3.5.3.1&nbsp;&nbsp;</span>prevent Setting WithCopyWarning in pandas</a></span></li><li><span><a href=\"#Show-max-rows-and-columns-inline-for-pandas-dataframes\" data-toc-modified-id=\"Show-max-rows-and-columns-inline-for-pandas-dataframes-3.5.3.2\"><span class=\"toc-item-num\">3.5.3.2&nbsp;&nbsp;</span>Show max rows and columns inline for pandas dataframes</a></span></li><li><span><a href=\"#Read-in-series-of-csv-files-and-concatenate-into-one-csv-file\" data-toc-modified-id=\"Read-in-series-of-csv-files-and-concatenate-into-one-csv-file-3.5.3.3\"><span class=\"toc-item-num\">3.5.3.3&nbsp;&nbsp;</span>Read in series of csv files and concatenate into one csv file</a></span></li></ul></li></ul></li><li><span><a href=\"#Visualisations\" data-toc-modified-id=\"Visualisations-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Visualisations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Graph-Database-Packages\" data-toc-modified-id=\"Graph-Database-Packages-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>Graph Database Packages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gephi-(graph-creator)-and-Yed-(graph-visualiser)\" data-toc-modified-id=\"Gephi-(graph-creator)-and-Yed-(graph-visualiser)-3.6.1.1\"><span class=\"toc-item-num\">3.6.1.1&nbsp;&nbsp;</span>Gephi (graph creator) and Yed (graph visualiser)</a></span></li></ul></li><li><span><a href=\"#Using-ipywidgets-to-give-interactive-graphs\" data-toc-modified-id=\"Using-ipywidgets-to-give-interactive-graphs-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>Using ipywidgets to give interactive graphs</a></span></li></ul></li><li><span><a href=\"#Productionisation-of-code\" data-toc-modified-id=\"Productionisation-of-code-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Productionisation of code</a></span><ul class=\"toc-item\"><li><span><a href=\"#python-pip-package-method-with-GitHub\" data-toc-modified-id=\"python-pip-package-method-with-GitHub-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>python pip package method with GitHub</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-pip-package\" data-toc-modified-id=\"Create-pip-package-3.7.1.1\"><span class=\"toc-item-num\">3.7.1.1&nbsp;&nbsp;</span>Create pip package</a></span></li></ul></li><li><span><a href=\"#python-pip-package-method-with-Docker,-TeamCity-and-Artifactory\" data-toc-modified-id=\"python-pip-package-method-with-Docker,-TeamCity-and-Artifactory-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>python pip package method with Docker, TeamCity and Artifactory</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-links\" data-toc-modified-id=\"Useful-links-3.7.2.1\"><span class=\"toc-item-num\">3.7.2.1&nbsp;&nbsp;</span>Useful links<br></a></span></li></ul></li><li><span><a href=\"#Example-usage-(stock_data_collector)\" data-toc-modified-id=\"Example-usage-(stock_data_collector)-3.7.3\"><span class=\"toc-item-num\">3.7.3&nbsp;&nbsp;</span>Example usage (stock_data_collector)</a></span></li><li><span><a href=\"#Deploy-package-from-Cloudera-to-artifactory\" data-toc-modified-id=\"Deploy-package-from-Cloudera-to-artifactory-3.7.4\"><span class=\"toc-item-num\">3.7.4&nbsp;&nbsp;</span>Deploy package from Cloudera to artifactory</a></span></li><li><span><a href=\"#AWS-Batch-method\" data-toc-modified-id=\"AWS-Batch-method-3.7.5\"><span class=\"toc-item-num\">3.7.5&nbsp;&nbsp;</span>AWS Batch method</a></span><ul class=\"toc-item\"><li><span><a href=\"#Docker-container-of-code\" data-toc-modified-id=\"Docker-container-of-code-3.7.5.1\"><span class=\"toc-item-num\">3.7.5.1&nbsp;&nbsp;</span>Docker container of code</a></span></li><li><span><a href=\"#TeamCity\" data-toc-modified-id=\"TeamCity-3.7.5.2\"><span class=\"toc-item-num\">3.7.5.2&nbsp;&nbsp;</span>TeamCity</a></span></li><li><span><a href=\"#Octopus\" data-toc-modified-id=\"Octopus-3.7.5.3\"><span class=\"toc-item-num\">3.7.5.3&nbsp;&nbsp;</span>Octopus</a></span></li><li><span><a href=\"#AWS-Lambda\" data-toc-modified-id=\"AWS-Lambda-3.7.5.4\"><span class=\"toc-item-num\">3.7.5.4&nbsp;&nbsp;</span>AWS Lambda</a></span></li><li><span><a href=\"#AWS-Batch\" data-toc-modified-id=\"AWS-Batch-3.7.5.5\"><span class=\"toc-item-num\">3.7.5.5&nbsp;&nbsp;</span>AWS Batch</a></span></li></ul></li></ul></li><li><span><a href=\"#Communications-packages\" data-toc-modified-id=\"Communications-packages-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Communications packages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-ZMQ-pub/sub-model-for-message-transfer\" data-toc-modified-id=\"Using-ZMQ-pub/sub-model-for-message-transfer-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>Using ZMQ pub/sub model for message transfer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-links\" data-toc-modified-id=\"Useful-links-3.8.1.1\"><span class=\"toc-item-num\">3.8.1.1&nbsp;&nbsp;</span>Useful links</a></span></li><li><span><a href=\"#Simple-pub/sub-example-using-sockets\" data-toc-modified-id=\"Simple-pub/sub-example-using-sockets-3.8.1.2\"><span class=\"toc-item-num\">3.8.1.2&nbsp;&nbsp;</span>Simple pub/sub example using sockets</a></span></li></ul></li><li><span><a href=\"#Automatic-email-updates-in-python-using-smtlib\" data-toc-modified-id=\"Automatic-email-updates-in-python-using-smtlib-3.8.2\"><span class=\"toc-item-num\">3.8.2&nbsp;&nbsp;</span>Automatic email updates in python using smtlib</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-Links\" data-toc-modified-id=\"Useful-Links-3.8.2.1\"><span class=\"toc-item-num\">3.8.2.1&nbsp;&nbsp;</span>Useful Links</a></span></li><li><span><a href=\"#Import-necessaary-libraries-including-smtlib\" data-toc-modified-id=\"Import-necessaary-libraries-including-smtlib-3.8.2.2\"><span class=\"toc-item-num\">3.8.2.2&nbsp;&nbsp;</span>Import necessaary libraries including smtlib</a></span></li><li><span><a href=\"#Create-the-MIMEMultipart-message-object-and-load-it-with-appropriate-headers-for-From,-To,-and-Subject-fields\" data-toc-modified-id=\"Create-the-MIMEMultipart-message-object-and-load-it-with-appropriate-headers-for-From,-To,-and-Subject-fields-3.8.2.3\"><span class=\"toc-item-num\">3.8.2.3&nbsp;&nbsp;</span>Create the MIMEMultipart message object and load it with appropriate headers for From, To, and Subject fields</a></span></li><li><span><a href=\"#Set-up-the-SMTP-server-and-log-into-your-account\" data-toc-modified-id=\"Set-up-the-SMTP-server-and-log-into-your-account-3.8.2.4\"><span class=\"toc-item-num\">3.8.2.4&nbsp;&nbsp;</span>Set up the SMTP server and log into your account</a></span></li><li><span><a href=\"#Send-the-message-using-the-SMTP-server-object\" data-toc-modified-id=\"Send-the-message-using-the-SMTP-server-object-3.8.2.5\"><span class=\"toc-item-num\">3.8.2.5&nbsp;&nbsp;</span>Send the message using the SMTP server object</a></span></li></ul></li></ul></li><li><span><a href=\"#Web-Server-(Using-Dash,-Flask,-Gunicorn,-Nginz)\" data-toc-modified-id=\"Web-Server-(Using-Dash,-Flask,-Gunicorn,-Nginz)-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Web Server (Using Dash, Flask, Gunicorn, Nginz)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-Links-to-be-checked-and-moved-where-necessary\" data-toc-modified-id=\"Useful-Links-to-be-checked-and-moved-where-necessary-3.9.1\"><span class=\"toc-item-num\">3.9.1&nbsp;&nbsp;</span>Useful Links to be checked and moved where necessary</a></span></li></ul></li><li><span><a href=\"#Web-Server-(Using-Dash,-Flask,-Gunicorn,-Nginz)\" data-toc-modified-id=\"Web-Server-(Using-Dash,-Flask,-Gunicorn,-Nginz)-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;</span>Web Server (Using Dash, Flask, Gunicorn, Nginz)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Useful-Links\" data-toc-modified-id=\"Useful-Links-3.10.1\"><span class=\"toc-item-num\">3.10.1&nbsp;&nbsp;</span>Useful Links</a></span></li><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-3.10.2\"><span class=\"toc-item-num\">3.10.2&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#python-application-(like-Dash/Flask)\" data-toc-modified-id=\"python-application-(like-Dash/Flask)-3.10.2.1\"><span class=\"toc-item-num\">3.10.2.1&nbsp;&nbsp;</span>python application (like Dash/Flask)</a></span></li><li><span><a href=\"#A-WSGI-application-server-(like-Gunicorn)\" data-toc-modified-id=\"A-WSGI-application-server-(like-Gunicorn)-3.10.2.2\"><span class=\"toc-item-num\">3.10.2.2&nbsp;&nbsp;</span>A WSGI application server (like Gunicorn)</a></span></li><li><span><a href=\"#A-web-server-(like-nginx)\" data-toc-modified-id=\"A-web-server-(like-nginx)-3.10.2.3\"><span class=\"toc-item-num\">3.10.2.3&nbsp;&nbsp;</span>A web server (like nginx)</a></span></li></ul></li><li><span><a href=\"#Step-by-step-guide-to-building-a-production-ready-web-server\" data-toc-modified-id=\"Step-by-step-guide-to-building-a-production-ready-web-server-3.10.3\"><span class=\"toc-item-num\">3.10.3&nbsp;&nbsp;</span>Step by step guide to building a production ready web server</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prerequisites\" data-toc-modified-id=\"Prerequisites-3.10.3.1\"><span class=\"toc-item-num\">3.10.3.1&nbsp;&nbsp;</span>Prerequisites</a></span></li><li><span><a href=\"#Installing-the-Components-from-the-Ubuntu-Repositories\" data-toc-modified-id=\"Installing-the-Components-from-the-Ubuntu-Repositories-3.10.3.2\"><span class=\"toc-item-num\">3.10.3.2&nbsp;&nbsp;</span>Installing the Components from the Ubuntu Repositories</a></span></li><li><span><a href=\"#Creating-a-Python-Virtual-Environment\" data-toc-modified-id=\"Creating-a-Python-Virtual-Environment-3.10.3.3\"><span class=\"toc-item-num\">3.10.3.3&nbsp;&nbsp;</span>Creating a Python Virtual Environment</a></span></li><li><span><a href=\"#Setting-Up-a-Flask-Application\" data-toc-modified-id=\"Setting-Up-a-Flask-Application-3.10.3.4\"><span class=\"toc-item-num\">3.10.3.4&nbsp;&nbsp;</span>Setting Up a Flask Application</a></span></li><li><span><a href=\"#Creating-a-Sample-App\" data-toc-modified-id=\"Creating-a-Sample-App-3.10.3.5\"><span class=\"toc-item-num\">3.10.3.5&nbsp;&nbsp;</span>Creating a Sample App</a></span></li><li><span><a href=\"#Setup-WSGI-Endpoint\" data-toc-modified-id=\"Setup-WSGI-Endpoint-3.10.3.6\"><span class=\"toc-item-num\">3.10.3.6&nbsp;&nbsp;</span>Setup WSGI Endpoint</a></span></li><li><span><a href=\"#Configuring-Gunicorn\" data-toc-modified-id=\"Configuring-Gunicorn-3.10.3.7\"><span class=\"toc-item-num\">3.10.3.7&nbsp;&nbsp;</span>Configuring Gunicorn</a></span></li></ul></li><li><span><a href=\"#Configuring-Nginx-to-proxy-requests\" data-toc-modified-id=\"Configuring-Nginx-to-proxy-requests-3.10.4\"><span class=\"toc-item-num\">3.10.4&nbsp;&nbsp;</span>Configuring Nginx to proxy requests</a></span><ul class=\"toc-item\"><li><span><a href=\"#Securing-the-Application-with-SSL-encryption-from-Lets-Encrypt\" data-toc-modified-id=\"Securing-the-Application-with-SSL-encryption-from-Lets-Encrypt-3.10.4.1\"><span class=\"toc-item-num\">3.10.4.1&nbsp;&nbsp;</span>Securing the Application with SSL encryption from Lets Encrypt</a></span></li><li><span><a href=\"#Log-out-button-link\" data-toc-modified-id=\"Log-out-button-link-3.10.4.2\"><span class=\"toc-item-num\">3.10.4.2&nbsp;&nbsp;</span>Log out button link</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Scala-and-Spark\" data-toc-modified-id=\"Scala-and-Spark-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Scala and Spark</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scalable-data-science\" data-toc-modified-id=\"Scalable-data-science-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Scalable data science</a></span></li><li><span><a href=\"#Useful-links\" data-toc-modified-id=\"Useful-links-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Useful links</a></span></li><li><span><a href=\"#Important-note-on-compatible-verisons\" data-toc-modified-id=\"Important-note-on-compatible-verisons-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Important note on compatible verisons</a></span></li><li><span><a href=\"#Initial-setup-linux\" data-toc-modified-id=\"Initial-setup-linux-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Initial setup linux</a></span><ul class=\"toc-item\"><li><span><a href=\"#Java\" data-toc-modified-id=\"Java-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Java</a></span></li><li><span><a href=\"#Spark\" data-toc-modified-id=\"Spark-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Spark</a></span></li><li><span><a href=\"#Hadoop\" data-toc-modified-id=\"Hadoop-4.4.3\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;</span>Hadoop</a></span></li><li><span><a href=\"#Scala\" data-toc-modified-id=\"Scala-4.4.4\"><span class=\"toc-item-num\">4.4.4&nbsp;&nbsp;</span>Scala</a></span></li><li><span><a href=\"#SBT\" data-toc-modified-id=\"SBT-4.4.5\"><span class=\"toc-item-num\">4.4.5&nbsp;&nbsp;</span>SBT</a></span></li><li><span><a href=\"#Check-installations\" data-toc-modified-id=\"Check-installations-4.4.6\"><span class=\"toc-item-num\">4.4.6&nbsp;&nbsp;</span>Check installations</a></span></li></ul></li><li><span><a href=\"#Initial-setup-Windows\" data-toc-modified-id=\"Initial-setup-Windows-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Initial setup Windows</a></span><ul class=\"toc-item\"><li><span><a href=\"#Java\" data-toc-modified-id=\"Java-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Java</a></span></li><li><span><a href=\"#Spark\" data-toc-modified-id=\"Spark-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>Spark</a></span></li><li><span><a href=\"#winutils-(Hadoop-Support)\" data-toc-modified-id=\"winutils-(Hadoop-Support)-4.5.3\"><span class=\"toc-item-num\">4.5.3&nbsp;&nbsp;</span>winutils (Hadoop Support)</a></span></li><li><span><a href=\"#Scala\" data-toc-modified-id=\"Scala-4.5.4\"><span class=\"toc-item-num\">4.5.4&nbsp;&nbsp;</span>Scala</a></span></li><li><span><a href=\"#SBT\" data-toc-modified-id=\"SBT-4.5.5\"><span class=\"toc-item-num\">4.5.5&nbsp;&nbsp;</span>SBT</a></span></li><li><span><a href=\"#Define-all-the-necessary-path-variables\" data-toc-modified-id=\"Define-all-the-necessary-path-variables-4.5.6\"><span class=\"toc-item-num\">4.5.6&nbsp;&nbsp;</span>Define all the necessary path variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#System-and-user-variables-explained\" data-toc-modified-id=\"System-and-user-variables-explained-4.5.6.1\"><span class=\"toc-item-num\">4.5.6.1&nbsp;&nbsp;</span>System and user variables explained</a></span></li></ul></li></ul></li><li><span><a href=\"#Check-installation-of-Spark\" data-toc-modified-id=\"Check-installation-of-Spark-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Check installation of Spark</a></span></li><li><span><a href=\"#IDE-Setup\" data-toc-modified-id=\"IDE-Setup-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>IDE Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#intellij-IDEA\" data-toc-modified-id=\"intellij-IDEA-4.7.1\"><span class=\"toc-item-num\">4.7.1&nbsp;&nbsp;</span>intellij IDEA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Windows-install\" data-toc-modified-id=\"Windows-install-4.7.1.1\"><span class=\"toc-item-num\">4.7.1.1&nbsp;&nbsp;</span>Windows install</a></span></li><li><span><a href=\"#Linux-install\" data-toc-modified-id=\"Linux-install-4.7.1.2\"><span class=\"toc-item-num\">4.7.1.2&nbsp;&nbsp;</span>Linux install</a></span></li></ul></li><li><span><a href=\"#Polynote\" data-toc-modified-id=\"Polynote-4.7.2\"><span class=\"toc-item-num\">4.7.2&nbsp;&nbsp;</span>Polynote</a></span></li><li><span><a href=\"#Zeppelin-notebooks\" data-toc-modified-id=\"Zeppelin-notebooks-4.7.3\"><span class=\"toc-item-num\">4.7.3&nbsp;&nbsp;</span>Zeppelin notebooks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup-for-local-zeppelin-(without-docker)\" data-toc-modified-id=\"Setup-for-local-zeppelin-(without-docker)-4.7.3.1\"><span class=\"toc-item-num\">4.7.3.1&nbsp;&nbsp;</span>Setup for local zeppelin (without docker)</a></span></li><li><span><a href=\"#Convert-between-databricks,-JSON-and-Zeppelin-notebook-files-using-Pinot\" data-toc-modified-id=\"Convert-between-databricks,-JSON-and-Zeppelin-notebook-files-using-Pinot-4.7.3.2\"><span class=\"toc-item-num\">4.7.3.2&nbsp;&nbsp;</span>Convert between databricks, JSON and Zeppelin notebook files using Pinot</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Sagemath\" data-toc-modified-id=\"Sagemath-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Sagemath</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Qualifications\n",
    "\n",
    "## AWS Certified Machine Learning Specialty\n",
    "\n",
    "### Useful Links\n",
    "\n",
    "#### Official exam page and AWS prep material<br>\n",
    "https://aws.amazon.com/certification/certified-machine-learning-specialty/<br>\n",
    "https://www.aws.training/Certification<br>\n",
    "https://aws.amazon.com/training/learning-paths/machine-learning/exam-preparation/<br>\n",
    "https://aws.amazon.com/training/learning-paths/machine-learning/data-scientist/<br>\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf<br>\n",
    "#### Linux Academy gives you access to an AWS console to participate in interacgtive labs<br>\n",
    "https://linuxacademy.com/course/aws-certified-machine-learning-specialty/<br>\n",
    "#### A cloud guru has an excellent exam simulator to practice exam type questions<br>\n",
    "https://learn.acloud.guru/course/aws-certified-machine-learning-specialty/dashboard<br>\n",
    "#### Useful blog on exam preparation<br>\n",
    "https://blog.thecloudtutor.com/2019/03/18/Passing-the-AWS-Certified-Machine-Learning-Specialty-Exam-MLS-C01.html<br>\n",
    "https://medium.com/@javier.ramos1/aws-machine-learning-certification-exam-tips-2a7679a83e73<br>\n",
    "#### crash course from o'reilly online learning along with associated links and material<br>\n",
    "https://www.oreilly.com/library/view/aws-certified-machine/9780135556597/<br>\n",
    "https://learning.oreilly.com/live-training/courses/aws-machine-learning-specialty-certification-crash-course/0636920259589/<br>\n",
    "https://github.com/noahgift/aws-ml-guide<br>\n",
    "https://noahgift.github.io/aws-ml-guide/intro<br>\n",
    "https://www.qwiklabs.com/quests/5<Br>\n",
    "https://www.oreilly.com/library/view/aws-certified-machine/9780135556597/<br>\n",
    "    \n",
    "## CRT020 Databricks Certified Associate Developer for Apache Spark 2.4 with Scala 2.11\n",
    "\n",
    "### Useful Links\n",
    "#### Official exam page<br>\n",
    "https://academy.databricks.com/category/certifications<br>\n",
    "#### Exam booking details<br>\n",
    "https://www.kryteriononline.com/sites/default/files/docs/PreparingForYourExam.pdf<br>\n",
    "https://go.proctoru.com/students/order<br>\n",
    "#### Study guide<br>\n",
    "https://www.linkedin.com/pulse/spark-simplified-certification-study-guide-raki-rahman/\n",
    "#### Blog on linkedin with detailed prep notes<br>\n",
    "https://www.linkedin.com/pulse/all-you-need-clear-crt020-databricks-certified-associate-kumar<br>\n",
    "https://www.linkedin.com/pulse/spark-simplified-certification-study-guide-raki-rahman/\n",
    "#### Useful FAQ answers for exam\n",
    "https://forums.databricks.com/questions/20492/has-anyone-taken-crt020-databricks-certified-assoc.html#answer-container-20719<br>\n",
    "https://forums.databricks.com/questions/29588/when-taking-the-2019-crt020-scalaspark-certificati.html<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Box\n",
    "Steps to build linux ubuntu virtual machine:\n",
    "   \n",
    "1) Enable virtualisation on bios<br>\n",
    "2) Download latest version of Ubuntu (64 bit version) as disk image from https://ubuntu.com/download/desktop<br>\n",
    "3) Download latest version of Oracle VirtualBox from https://www.virtualbox.org/ and install.<br>\n",
    "4) Setup virtual machine using VirtualBox using Ubuntu disk image as chosen OS.<br>\n",
    "5) Install VirtualBox Guest-additions. __Install Guest Additions 5.2.4 manually as 6.0 does not work with symbolic      links to host OS.__ \n",
    "   To install manually download the iso file from https://download.virtualbox.org/virtualbox/5.2.0_RC1/ \n",
    "   and then choose it in the mount in Virtualbox.\n",
    "\n",
    "<img src=\"media/vbox_1.png\">\n",
    "\n",
    "__Enabling best performance for full screen on second monitor__<br>\n",
    "In VB Manager---settings---display ensure:\n",
    "Video memory=128MB\n",
    "\n",
    "__Enabling copy and paste__<br>\n",
    "Install VirtualBox guest additions<br>\n",
    "In VM go to Devices --- Insert Guest Additions CD image.<br>\n",
    "Then go to devices---shared clipboard and set to bidirectional<br>\n",
    "Then go to devices---drag and drop and set to bidirectional<br>\n",
    "\n",
    "\n",
    "__Manually mount shared folder__<br>\n",
    "Sudo mount -t vboxsf SHARED_FOLDER_NAME MOUNT_LOCATION\n",
    "\n",
    "\n",
    "__Allow access to shared folder from linux__<br>\n",
    "Add user to group<br>\n",
    "sudo usermod -aG vboxsf username<br>\n",
    "\n",
    "__Change home directory to shared folder__\n",
    "Add below line to .bashrc file\n",
    "`cd /home/martin/shared_folder`\n",
    "\n",
    "__Allow drag and drop__<br>\n",
    "At top left of VM window Go to Devices then Drag and Drop then select Bidirectional\n",
    "\n",
    "__Install dpkg and associated packages__<br>\n",
    "`sudo apt-get install dpkg`<br>\n",
    "`sudo apt-get install virtualbox-guest-gkms virtualbox-guest-utils virtualbox-guest-x11`<br>\n",
    "\n",
    "### Using vboxmanage to configure virtualbox<br>\n",
    "\n",
    "__IMPORTANT NOTES:__<br>\n",
    "1.Guest Additions 6.0 does not work with below commands use Guest Additions 5.2.4<br>\n",
    "2.Make sure virtualbox is set to permanently run as an administrator<br>\n",
    "3.vboxmanage and VBoxManage both work, its not case sensitive<br>\n",
    "\n",
    "\n",
    "\n",
    "__Allow symbolic links in shared folder (For virtualenv etc)__<br>\n",
    "In command prompt in windows change to directory<br>\n",
    "`C:/Program Files/Oracle/VirtualBox`<br>\n",
    "Then run command below replacing `VM_NAME` with name of virtualbox machine.\n",
    "(part in bold is full path to shared folder as it appears in virtualbox menu)\n",
    "\n",
    "\n",
    "`vboxmanage setextradata VM_NAME VBoxInternal2/SharedFoldersEnableSymlinksCreate/PATH_TO_SHARED_FOLDER 1`<br>\n",
    "<img src=\"media/vbox_2.png\">\n",
    "\n",
    "__If above code doesnt work try shared folder name with no filepath__\n",
    "`vboxmanage setextradata VM_NAME VBoxInternal2/SharedFoldersEnableSymlinksCreate/SHARED_FOLDER 1`\n",
    "<img src=\"media/vbox_3.png\">\n",
    "\n",
    "__To check current symbolic links__<br>\n",
    "`vboxmanage getextradata VM_NAME enumerate`\n",
    "<img src=\"media/vbox_4.png\"> \n",
    "__Example output__\n",
    "<img src=\"media/vbox_5.png\">\n",
    "\n",
    "__Change location of shared folder__<br>\n",
    "`Vboxmanage guestproperty set VM_NAME /VirtualBox/GuestAdd/SharedFolders/MountDir /home/user/`\n",
    "<img src=\"media/vbox_6.png\"><br>\n",
    "__Remove sf_ prefix from shared folder__<br>\n",
    "`vboxmanage guestproperty set VM_NAME /VirtualBox/GuestAdd/SharedFolders/MountPrefix /`\n",
    "<img src=\"media/vbox_7.png\"><br>\n",
    "\n",
    "__Check location and prefix shared folder details__<br>\n",
    "`Vboxmanage guestproperty enumerate VM_NAME`<br>\n",
    "<img src=\"media/vbox_8.png\">\n",
    "__Example output__\n",
    "<img src=\"media/vbox_9.png\">\n",
    "\n",
    "### Increase size of VM partition\n",
    "\n",
    "Follow instructions on below link\n",
    "http://derekmolloy.ie/resize-a-virtualbox-disk/#prettyPhoto\n",
    "\n",
    "Make sure to connect correct new .vdi file afterwards to your VM as below.\n",
    "\n",
    "<img src=\"media/vbox_10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup PATH variable correctly<br>\n",
    "__PATH__: A list of directories that the system will check when looking for binary files and executables for packages. When a user types in a command, the system will check directories in this order for the executable.<br>\n",
    "https://www.digitalocean.com/community/tutorials/how-to-read-and-set-environmental-and-shell-variables-on-a-linux-vps<br>\n",
    "\n",
    "Vim environment file in etc folder and check if contents same as below<br>\n",
    "`PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\"`<br>\n",
    "\n",
    "## Immediate activation of any path updates without closing session<br>\n",
    "`source /etc/environment && export PATH`<br>\n",
    "\n",
    "Can also update the __PATH__ in `.profile` or `.bashrc` files but environment best way to do it<br>\n",
    "\n",
    "## Symbolic links\n",
    "__Create symbolic link to python 3__<br>\n",
    "`Sudo ln -s /usr/python3 /usr/bin/python`<br>\n",
    "<pre>          TARGET       LINK_NAME</pre>\n",
    "\n",
    "<font color=red>DANGER:</font> Don not symbolic link ‘python’ to ‘python3’ as linux system needs python2 for some processes and its uses python as its command syntax. Instead use as alias in .bashrc like\n",
    "alias python=python3\n",
    "\n",
    "\n",
    "__Create symbolic link to virtual env activation file__\n",
    "ln -s  ~/Desktop/virtualenvs/py3/bin/activate  ~/py3_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linux Server\n",
    "\n",
    "__find job on certain port__<br>\n",
    "`lsof -t -i :PORT_NUMBER`\n",
    "\n",
    "__SCP copying__<br>\n",
    "copy files from local to remote<br>\n",
    "Note this copies to the root folder of the server as no filepath given after the address:\n",
    "`pscp -r FILEPATH_TO_ORIGINAL_FILE root@IP_ADDRESS:`\n",
    "\n",
    "copy files from remote to local<br>\n",
    "`pscp root@IP_ADDRESS:FILEPATH_TO_ORIGINAL_FILE FILEPATH_TO_DESTINATION:`\n",
    "\n",
    "__Adding Additional Users__<br>\n",
    "Important note: After any changes to sshd_config in below instructions ensure update is activated<br>\n",
    "`sudo service ssh reload`\n",
    "\n",
    "__Allow password authorization__<br>\n",
    "Go to `/etc/ssh/sshd_config` and update line below yes<br>\n",
    "`PasswordAuthentication yes`\n",
    "\n",
    "__Add new user called mmcgov__\n",
    "add new user and set no home directory as want same home directory as root. Specify home directory as root. \n",
    "Add any groups which contain the files they need to access and make sure to add them to sudo group.<br>\n",
    "`adduser --home /root --shell /bin/bash --no-create-home --ingroup GROUP_NAME --ingroup sudo`<br>\n",
    "\n",
    "•\t`adduser` is used to add a user<br>\n",
    "•\t`--home` specifies home directory which is where the user will be when they log in<br>\n",
    "•\t`--shell` is to specify the shell, by default it is usually just `/bin/sh` which is not as user friendly as `/bin/bash`<br>\n",
    "•\t`--no-create-home` will not create the home directory so you must use one that already exists<br>\n",
    "•\t`--ingroup` adds the user to specified group (see below need to create group first)<br>\n",
    "•\tthe last argument is the username<br>\n",
    "•\t__NOTE__: group is created in act of using chown so need to do this first before assigning group to user\n",
    "You will be asked to create password, you can hit return to skip rest of required information\n",
    "\n",
    "__Add user to specific group__\n",
    "`sudo usermod -a -G GROUPNAME USERNAME`\n",
    "\n",
    "__check which groups a user in__\n",
    "`groups USERNAME`\n",
    "\n",
    "__check which groups all user__\n",
    "`vim /etc/group`\n",
    "\n",
    "__Login as new user and setup__\n",
    "ssh as below, you will be prompted for password<br>\n",
    "`ssh USERNAME@ip_address`<br>\n",
    "go to home default directory and create new directory<br>  \n",
    "`~/.ssh`\n",
    "Create new file in this folder called `authorized_keys` and paste in `public key` from root profile. \n",
    "\n",
    "__Forbid password authentication__\n",
    "PasswordAuthentication no<br>\n",
    "\n",
    "__Check home directory for each user__\n",
    "Look for all users in `/etc/passwd`<br>\n",
    "Alternatively use below command<br>\n",
    "`awk -F: '{print$1}' /etc/passwd`\n",
    "\n",
    "__Remove a user__<br>\n",
    "`userdel USER_NAME`\n",
    "\n",
    "__Granting access for user to certain directories__<br>\n",
    "First change group of files you want to share to new group which new user will be part of<br>\n",
    "`chown     root:gemini    chosen_folder/`<br>\n",
    "first name is owner (root) second is group (gemini) last part is file directory. \n",
    "\n",
    "__grant access recursively to directories__<br>\n",
    "`chmod -R 755 root/`\n",
    "\n",
    "__Codes for access explained__\n",
    "7 = 4+2+1 (read/write/execute)<br>\n",
    "6 = 4+2 (read/write)<br>\n",
    "5 = 4+1 (read/execute)<br>\n",
    "4 = 4 (read)<br>\n",
    "3 = 2+1 (write/execute)<br>\n",
    "2 = 2 (write)<br>\n",
    "1 = 1 (execute)<br>\n",
    "\n",
    "__Alternative symbolic method of changing chmod explained__<br>\n",
    "The first and probably easiest way is the relative (or symbolic) method, which lets you specify access classes and types with single letter abbreviations. A chmod command with this form of syntax consists of at least three parts from the following lists:\n",
    "\n",
    "__Access Class__<br>\n",
    "u(User), g(group), o(other), a(all:u, g and o)<br>\n",
    "__Operator__<br>\n",
    "+(add access), -(remove access), =(set exact access)<br>\n",
    "__Access__<br>\n",
    "r(read), w(write), x(execute)<br>\n",
    "<br>\n",
    "__Examples__<br>\n",
    "To add read access for all on testfile<br>\n",
    "`chmod a+r testfile`<br>\n",
    "\n",
    "\n",
    "To remove read and write access for user and others on testfile<br>\n",
    "`chmod uo-w testfile`<br>\n",
    "\n",
    "To explicitly set read access for other on testfile<br>\n",
    "`chmod o=r testfile`<br>\n",
    "\n",
    "For more help/details on chmod<br>\n",
    "`man chmod`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSH Notes\n",
    "\n",
    "__useful links__<br>\n",
    "https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-18-04<br>\n",
    "\n",
    "__log onto remote server__<br>\n",
    "`ssh root@ip_address`\n",
    "\n",
    "__Create ssh key__\n",
    "`ssh-keygen -t rsa -b 4096 -C \"USER@EMAIL\"`<br>\n",
    "keys are saved at `~/.ssh`\n",
    "\n",
    "__view contents of ssh file to copy key__<br>\n",
    "`cat ~/.ssh/id_rsa.pub`\n",
    "\n",
    "Once key copied to server should be able to log in without need for password\n",
    "\n",
    "__Setting up ssh on second laptop__<br>\n",
    "Copy .ssh file from old home directory to home directory on new computer<br>\n",
    "ensure permissions inside .ssh are as follow<br>\n",
    "id_rsa = rw,-,-  (sudo chmod 6,0,0)<br>\n",
    "id_rsa.pub=rw,r,r (sudo chmod 6,4,4)<br>\n",
    "known_hosts=rw,r,r (sudo chmod 6,4,4)<br>\n",
    "\n",
    "\n",
    "__Using ssh as localhost__<br>\n",
    "`ssh-keygen -t rsa -b 4096 -C \"USER@EMAIL\"`<br>\n",
    "`cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys`<br>\n",
    "`chmod 0600 ~/.ssh/authorized_keys`<br>\n",
    "Then try:\n",
    "`ssh localhost`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Bugs\n",
    "### write on sudo owned file when did’nt use sudo to open<br>\n",
    "`:w ! sudo tee %`\n",
    "\n",
    "### Colour scheme<br>\n",
    "Restore defaults<br> \n",
    "`gsettings reset org.gnome.desktop.interface gtk-theme`<br>\n",
    "`gsettings reset org.gnome.desktop.interface icon-theme`<br>\n",
    "\n",
    "### Apt-get stalling on waiting for headers<br>\n",
    "If apt-get update not working and stalling while waiting for headers try changing sources.list file in `etc/apt/` to different prefix instead of\n",
    "`deb http://ie.archive.ubuntu.com/ubuntu/ bionic main restricted`\n",
    "change to \n",
    "`deb http://eng.archive.ubuntu.com/ubuntu/ bionic main restricted`\n",
    "If this does not fix then see below for changing default repo for linux\n",
    "\n",
    "\n",
    "### Problems with Linux software updater<br>\n",
    "Untick below repo as can cause problems with updates\n",
    "`https://packages.microsoft.com/ubuntu/16.04/mssql-server xenial main`\n",
    "only these 3 packages should be ticked\n",
    "`https://packages.microsoft.com/ubuntu/16.04/mssql-server-2017 xenial main(Source Code)`\n",
    "`http://dl.google.com/linux/chrome/deb/ stable main`\n",
    "`https://packages.microsoft.com/ubuntu/16.04/prod xenial main`\n",
    "Can check which repos not working by just running updater\n",
    "\n",
    "## Docker\n",
    "\n",
    "### Install docker\n",
    "`Pip install docker-compose`<br>\n",
    "`Pip install docker`<br>\n",
    "`Sudo apt install docker.io`<br>\n",
    "\n",
    "`sudo chmod +x /usr/local/bin/docker-compose`<br>\n",
    "`sudo usermod -aG docker USER_NAME`<br>\n",
    "\n",
    "### Docker commands\n",
    "(Vim into Makefile for basic commands)\n",
    "\n",
    "__Enter docker container__\n",
    "`./dockercmd.sh bash`<br>\n",
    "(Problems with docker command make dev bash then Cat dockercmd.sh and run single line of code without bash lines)<br>\n",
    "\n",
    "__Install packages in docker__\n",
    "Add following to Dockerfile for useful system packages. Can run once in the ‘make build’ and then comment out.\n",
    " `RUN apt-get update -y && apt-get -o Dpkg::Options::=\"--force-overwrite\"` \n",
    "\n",
    "\n",
    "      \n",
    "### Docker-Compose commands\n",
    "\n",
    "To start docker run this from inside docker directory<br>\n",
    "`Docker-compose up -d`<br>\n",
    "\n",
    "To stop docker<br>\n",
    "`Docker-compose down`<br>\n",
    "\n",
    "list size of containers, objects etc<br>\n",
    "`sudo du -h --max-depth=1`<br>\n",
    "Clean up docker related items<br>\n",
    "`docker system prune -a` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links\n",
    "Magic Commands<br>\n",
    "https://ipython.org/ipython-doc/3/interactive/magics.html#line-magics\n",
    "Coding tips<br>\n",
    "https://powerfulpython.com/safari-trainings/<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environments\n",
    "\n",
    "It is good practice to setup virtual environments for each project so that the requirements/dependencies can be keep isolated for that project and not becomne interwined with the base linux python version which is used for critical system processses.\n",
    "\n",
    "\n",
    "https://medium.com/@__pamaron__/understanding-and-use-python-virtualenvs-from-data-scientist-perspective-bfed61faeb3f<br>\n",
    "\n",
    "https://favoorr.github.io/2017/01/03/python3-6-virtualenvwrapper-problem-md/<br>\n",
    "\n",
    "\n",
    "### Install virtualenv and virtualenvwrapper<br>\n",
    "`Sudo pip install virtualenv`\n",
    "\n",
    "`sudo pip install virtualenvwrapper`\n",
    "\n",
    "Should really be installed via sudo to be available to all but if only local then can<br>\n",
    "`Pip install virtualenv`<br>\n",
    "`Pip install virtualenvwrapper`<br>\n",
    "\n",
    "In the case of virtualenv wrapper you also need to add the following lines to the .bashrc.<br>\n",
    "\n",
    "`export VIRTUALENVWRAPPER_VIRTUALENV=/usr/local/bin/virtualenv`<br>\n",
    "`source /usr/local/bin/virtualenvwrapper.sh`<br>\n",
    "\n",
    "__NOTE__<br>\n",
    "If having difficulties with symbolic links on shared folder of VM for example then use flag --always-copy to vreate a virtualenv without symlinks where all the executables are copied into the new env.<br>\n",
    "\n",
    "### Wrapper syntax<br>\n",
    "https://howchoo.com/g/nwewzjmzmjc/a-guide-to-python-virtual-environments-with-virtualenvwrapper\n",
    "\n",
    "__Create new virtualenv called py3 with python 3.6 installed__\n",
    "`mkvirtualenv -p /usr/bin/python3.6 py3`<br>\n",
    "This creates virtualenv in ~/.virtualenvs\n",
    "\n",
    "__To list all virtualenvs:__\n",
    "`lsvirtualenv`\n",
    "\n",
    "__to enter virtualenv:__\n",
    "`workon py3`\n",
    "\n",
    "__exit virtualenv__\n",
    "`deactivate`\n",
    "\n",
    "__remove virtualenv__\n",
    "`rmvirtualenv py3`\n",
    "\n",
    "\n",
    "### Virtualenv syntax<br>\n",
    "\n",
    "__create new environment__<br>\n",
    "`virtualenv ENVNAME`<br>\n",
    "\n",
    "__activate virtualenv__\n",
    "example for env named py2<br>\n",
    "`source ~/Desktop/virtualenvs/py2/bin/activate`<br>\n",
    "\n",
    "Create symbolic link to virtual env activation file to save typing full path<br>\n",
    "`ln -s  ~/Desktop/virtualenvs/py3/bin/activate  ~/py3_env`\n",
    "\n",
    "can then simply use:<br>\n",
    "`source ~/py3_env`\n",
    "\n",
    "__deactivate virtualenv__<br>\n",
    "`deactivate`\n",
    "\n",
    "install chosen python<br>\n",
    "eg py2 env with python2\n",
    "`virtualenv -p python2 py2`\n",
    "eg py3 env with python3\n",
    "`virtualenv -p python3 py3`\n",
    "\n",
    "install packages\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "example requirements.txt<br>\n",
    "numpy==1.15.3<br>\n",
    "pandas==0.23.4<br>\n",
    "jellyfish==0.6.1<br>\n",
    "sqlalchemy<br>\n",
    "screen<br>\n",
    "\n",
    "### Conda syntax\n",
    "\n",
    "__VIRTUAL ENVIRONMENT TIPS__\n",
    "    \n",
    "__to enter environment__ <Br>\n",
    "`source activate ENV NAME<br>\n",
    "\n",
    "__to exit__<br>\n",
    "`source deactivate ENV NAME`<br>\n",
    "\n",
    "__to create new environment__<br>\n",
    "`conda create --name ENV NAME`<br>\n",
    "\n",
    "__to list environments__<br>\n",
    "`conda info --envs<br>`\n",
    "\n",
    "__export environment__<br>\n",
    "`conda env export > FILENAME`<br>\n",
    "\n",
    "__create environment from file__<br>\n",
    "`conda env create -f FILENAME`<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Pip setup<br>\n",
    "sudo apt install update<br>\n",
    "sudo apt install python3-pip<br>\n",
    "\n",
    "__Pip install options__<br>\n",
    "`pip install cleanco`\n",
    "\n",
    "__Check if all pip packages up to date__<br>\n",
    "`pip list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U`\n",
    "\n",
    "__Package conflicts__<br>\n",
    "__Websocket - Package conflicts__\n",
    "Must have below versions for websocket and websocket-client\n",
    "`websocket==0.2.1`\n",
    "`websocket-client==0.44.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDE Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vim\n",
    "\n",
    "__Useful links__<br>\n",
    "Shougo/neocomplcache.vim: Ultimate auto-completion system for Vim.<br>\n",
    "https://github.com/Shougo/neocomplcache.vim<br>\n",
    "\n",
    "__Quick Setup Guide__<br>\n",
    "Use existing setup by using existing vimrc and bashrc and screenrc and .vim folder from templates<br>\n",
    "run PlugInstall inside vim<br>\n",
    "\n",
    "__powerline__\n",
    "install like below:<br>\n",
    "`sudo apt install powerline`\n",
    "`pip install powerline-shell`\n",
    "\n",
    "add below lines to .bashrc<br>\n",
    "\n",
    "`function _update_ps1() {`<br>\n",
    "     `    PS1=$(powerline-shell $?)`<br>\n",
    "`}`<br>\n",
    " \n",
    "`if [[ $TERM != linux && ! $PROMPT_COMMAND =~ _update_ps1 ]]; then`<br>\n",
    "   `  PROMPT_COMMAND=\"_update_ps1; $PROMPT_COMMAND\"`<br>\n",
    "`fi`<br>\n",
    "\n",
    "`wget https://github.com/Lokaltog/powerline/raw/develop/font/PowerlineSymbols.otf`<br> \n",
    "`wget https://github.com/Lokaltog/powerline/raw/develop/font/10-powerline-symbols.conf`<br>\n",
    "`sudo fc-cache -vf`<br>\n",
    "`sudo mv 10-powerline-symbols.conf /etc/fonts/conf.d/`<br>\n",
    "\n",
    "__flake8__<br>\n",
    "`mkdir -p ~/.vim/pack/flake8/start/`<br>\n",
    "`cd ~/.vim/pack/flake8/start/`<br>\n",
    "`git clone https://github.com/nvie/vim-flake8.git`<br>\n",
    "also run <br>\n",
    "`pip install flake8`<br>\n",
    "\n",
    "\n",
    "`General vim commands`<br>\n",
    "Find replace in lines 100-200<br>\n",
    "`:100,200s/find/replace/g`\n",
    "\n",
    "__Find replace global__\n",
    "`:%s/find/replace/g`\n",
    "\n",
    "__Setting up .vimrc file__ \n",
    "Useful Links<br>\n",
    "`https://dougblack.io/words/a-good-vimrc.html`\n",
    "`https://github.com/kien/ctrlp.vim#basic-usage`\n",
    "\n",
    "__Colourcodes__<br>\n",
    "`https://jonasjacek.github.io/colors/`\n",
    "\n",
    "__Using Vim-Plug__<br>\n",
    "`https://github.com/junegunn/vim-plug`\n",
    "`Typical commands from inside vim file`\n",
    "\n",
    "__Remove plugin__<br>\n",
    "To remove plugins simply comment line from .vimrc then run<br>\n",
    "`:PlugClean`\n",
    "\n",
    "__Install plugin__<br>\n",
    "download git into `~/.vim/plugged` (make sure in the new app folder there is a subfolder plugin with the actual vim file in it)<br>\n",
    "\n",
    "then add relevant line to `.vimrc` for example<br>\n",
    "Plug 'joshdick/onedark.vim'<br>\n",
    "\n",
    "Finally run<br>\n",
    "`:PlugInstall`<br>\n",
    "\n",
    "__Check which plugins are slowing down vim__<br>\n",
    "https://stackoverflow.com/questions/12213597/how-to-see-which-plugins-are-making-vim-slow\n",
    "\n",
    "`:profile start profile.log`<br>\n",
    "`:profile func *`<br>\n",
    "`\" At this point do slow actions`<br>\n",
    "`:profile pause`<br>\n",
    "`:noautocmd qall!`<br>\n",
    "`:set more | verbose function {function_name}` will show you function contents and where it is located\n",
    "\n",
    "\n",
    "__Example of awkward plugins__<br>\n",
    "__Pydict plugin__<br>\n",
    "`git clone https://github.com/rkulla/pydiction.git` in plugged folder\n",
    "move `python_pydiction.vim` into `~/.vim/plugged/pydiction/plugin`\n",
    "\n",
    "__ctrlp.vim Addin__<br>\n",
    "Install silversearcher<br>\n",
    "`sudo apt-get install silversearcher-ag`<br>\n",
    "Install ctrlp.vim<br>\n",
    "Clone the plugin into a separate directory:<br>\n",
    "`$ cd ~/.vim`<br>\n",
    "`$ git clone https://github.com/ctrlpvim/ctrlp.vim.git bundle/ctrlp.vim`<br>\n",
    "Add to your ~/.vimrc:<br>\n",
    "set runtimepath^=~/.vim/bundle/ctrlp.vim<br>\n",
    "Run at Vim's command line:<br>\n",
    ":helptags ~/.vim/bundle/ctrlp.vim/doc<br>\n",
    "Restart Vim and check :help ctrlp.txt for usage instructions and configuration details.<br>\n",
    "Set default ctrlp to PMRU<br>\n",
    "let g:ctrlp_map='<c-p>'<br>\n",
    "let g:ctrlp_cmd = 'CtrlPMRU'<br>\n",
    "Change root dir for xtrlp<br>\n",
    "noremap <C-a> :CtrlP /media/sf_linux_shared/<CR><br>\n",
    "Use ctrlp with splits<br>\n",
    "Once find file use ctrl-v for vertical split and ctrl-s for horizontal split\n",
    "\n",
    "\n",
    "Powerline<br>\n",
    "sudo apt install powerline<br>\n",
    "wget https://github.com/Lokaltog/powerline/raw/develop/font/PowerlineSymbols.otf<br>\n",
    "https://github.com/Lokaltog/powerline/raw/develop/font/10-powerline-symbols.conf<br>\n",
    "sudo fc-cache -vf<br>\n",
    "sudo mv 10-powerline-symbols.conf /etc/fonts/conf.d/<br>\n",
    "\n",
    "Extra notes on powerline<br>\n",
    "Download or clone file from :<br>\n",
    "https://github.com/powerline/powerline<br>\n",
    "\n",
    "bash version<br>\n",
    "It should automatically work for bashrc but if not add below to .bashrc. (NOTE BELOW LINES SLOW DOWN THE PROMPT A LOT SO SHOULD NOT BE USED UNLESS NO OTHER OPTION)<br>\n",
    "<br>\n",
    "\n",
    "`function _update_ps1() {`<br>\n",
    " `    PS1=$(powerline-shell $?)`<br>\n",
    "`}`<br>\n",
    " \n",
    "`if [[ $TERM != linux && ! $PROMPT_COMMAND =~ _update_ps1 ]]; then`<br>\n",
    "`     PROMPT_COMMAND=\"_update_ps1; $PROMPT_COMMAND\"`<br>\n",
    "`fi`<br>\n",
    "\n",
    "Vim version<br>\n",
    "Add following lines to .vimrc (changing filepath to where the powerline was downloaded to)<br>\n",
    "\n",
    "`\" powerline`<br>\n",
    "`set rtp+=/home/martin/linux_shared/training/initial_setup/linux/powerlinedevelop/powerline/bindings/vim`<br>\n",
    "                                                                                                                                                                                                         \n",
    "` \" Always show statusline`<br>\n",
    "` set laststatus=2`<br>\n",
    " \n",
    "` \" Use 256 colours (Use this setting only if your terminal supports 256 colours)`<br>\n",
    " `70 set t_Co=256`<br>\n",
    "\n",
    "__NERDTree__<br>\n",
    "https://github.com/scrooloose/nerdtree<br>\n",
    "Installation<br>\n",
    "`git clone https://github.com/scrooloose/nerdtree.git ~/.vim/bundle/nerdtree<br>`\n",
    "Then reload vim<br>\n",
    "\n",
    "__Ale__<br>\n",
    "Faster than syntastic as asynchronous<br>\n",
    "https://github.com/w0rp/ale#installation-with-vim-plug<br>\n",
    "\n",
    "__Linter - flake 8__<br>\n",
    "https://github.com/nvie/vim-flake8<br>\n",
    "Installation<br>\n",
    "Make sure you've installed the flake8 package.<br>\n",
    "If you use vim >= 8, install this plugin with:<br>\n",
    "`mkdir -p ~/.vim/pack/flake8/start/`<br>\n",
    "`cd ~/.vim/pack/flake8/start/`<br>\n",
    "`git clone https://github.com/nvie/vim-flake8.git`<Br>\n",
    "Otherwise, install `vim-pathogen` if you're not using it already. Then, simply put the contents of this repository in your `~/.vim/bundle` directory.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__splitting windows__<br>\n",
    "\n",
    "Horizontal split<br>\n",
    "`:sp FILENAME`<br>\n",
    "\n",
    "Vertical split<br>\n",
    "`:vsp FILENAME`<br>\n",
    "\n",
    "Resize splits<br>\n",
    "On creating new split<br>\n",
    "`:10sp ~/.zshrc`<br>\n",
    "\n",
    "Resizing to max size splits<br>\n",
    "Vim’s defaults are useful for changing split shapes:<br>\n",
    "`\"Max out the height of the current split`<br>\n",
    "`ctrl + w _`<br>\n",
    "\n",
    "\"Max out the width of the current split<br>\n",
    "`ctrl + w |`<br>\n",
    "\n",
    "\"Normalize all split sizes, which is very handy when resizing terminal<br>\n",
    "`ctrl + w =`<br>\n",
    "\n",
    "Resizing to variable size splits<br>\n",
    "There are several window commands that allow you to do this:<br>\n",
    "•\tCtrl+W +/-: increase/decrease height (ex. 20<C-w>+)<br>\n",
    "•\tCtrl+W >/<: increase/decrease width (ex. 30<C-w><)<br>\n",
    "•\tCtrl+W _: set height (ex. 50<C-w>_)<br>\n",
    "•\tCtrl+W |: set width (ex. 50<C-w>|)<br>\n",
    "•\tCtrl+W =: equalize width and height of all windows<br>\n",
    "See also: :help CTRL-W<br>\n",
    "\n",
    "More split manipulation<br>\n",
    "\"Swap top/bottom or left/right split<br>\n",
    "Ctrl+W R<br>\n",
    "\n",
    "\"Break out current window into a new tabview<br>\n",
    "Ctrl+W T<br>\n",
    "\n",
    "\"Close every window in the current tabview but the current one<br>\n",
    "Ctrl+W o<br>\n",
    "\n",
    "https://robots.thoughtbot.com/vim-splits-move-faster-and-more-naturally\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter\n",
    "\n",
    "__Access docs in jupyter__<br>\n",
    "`Shift tab docs`<br>\n",
    "__Full docs__<br>\n",
    "`Shift tabx4`<br>\n",
    "Or ? after the function<br>\n",
    "?? gives source code<br>\n",
    "\n",
    "\n",
    "__Use in virtualenv__<br>\n",
    "From inside a virtualenv install ipykernel and create new kernel<br>\n",
    "`pip install ipykernel`<br>\n",
    "`ipython kernel install --user --name=py3`<br>\n",
    "\n",
    "\n",
    "\n",
    "__Jupyter notebook themes__<br>\n",
    "Link to github (More detailed notes on github link)<br>\n",
    "https://github.com/dunovank/jupyter-themes/blob/master/README.md<br>\n",
    "\n",
    "__Install themes package__<br>\n",
    "`Pip install jupyterthemes`<br>\n",
    "upgrade to latest version<br>\n",
    "`pip install --upgrade jupyterthemes`<br>\n",
    "List themes<br>\n",
    "`Jt -l`<br>\n",
    "\n",
    "Available themes are:<br>\n",
    "Chesterish<br>\n",
    "Grade3<br>\n",
    "Gruvboxd<br>\n",
    "Gruvboxl<br>\n",
    "Monokai<br>\n",
    "Oceans16<br>\n",
    "Onedork<br>\n",
    "Solarized<br>\n",
    "solarizedl<br>\n",
    "\n",
    "\n",
    "__Change theme__<br>\n",
    "`Jt -t chesterish`\n",
    "\n",
    "__Chosen layout__\n",
    "`jt -t chesterish -fs 110 -ofs 10 -tfs 11 -nfs 125 -cellw 88% -T -N -kl -cursc r`\n",
    "`jt -t chesterish -fs 100 -ofs 10 -tfs 11 -nfs 100 -cellw 88% -T -N -kl -cursc r -dfonts`\n",
    "\n",
    "fs=code font size<br>\n",
    "tfs=text font size<br>\n",
    "nfs=notebook font size<br>\n",
    "cellw=cell width<br>\n",
    "cursc r  = cursor colour red<br>\n",
    "T = Toolbar<br>\n",
    "N = Name<br>\n",
    "KL = Kernel logo<br>\n",
    "\n",
    "Ensure graphs match theme\n",
    "Pro-tip: Include the following two lines in `~/.ipython/profile_default/startup/startup.ipy` file to set plotting style automatically whenever you start a notebook:\n",
    "\n",
    "import jtplot submodule from jupyterthemes<br>\n",
    "`from jupyterthemes import jtplot`<br>\n",
    "\n",
    "currently installed theme will be used to set plot style if no arguments provided\n",
    "`jtplot.style()`<br>\n",
    "\n",
    "\n",
    "If running jupyter in virtualenv  then run this inside a notebook once usually just first one unless want to specify fonts etc) all new notebooks should be fine<br>\n",
    "\n",
    "import jtplot module in notebook<br>\n",
    "`from jupyterthemes import jtplot`<br>\n",
    "\n",
    "choose which theme to inherit plotting style from<br>\n",
    "`onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd`<br>\n",
    "Example<br>\n",
    "`jtplot.style(theme='onedork')`\n",
    "\n",
    "set \"context\" (paper, notebook, talk, poster)<br>\n",
    "scale font-size of ticklabels, legend, etc.<br>\n",
    "remove spines from x and y axes and make grid dashed<br>\n",
    "`jtplot.style(context='talk', fscale=1.4, spines=False, gridlines='--')`\n",
    "\n",
    "turn on X- and Y-axis tick marks (default=False)<br>\n",
    "turn off the axis grid lines (default=True)<br>\n",
    "and set the default figure size<br>\n",
    "`jtplot.style(ticks=True, grid=False, figsize=(6, 4.5))`\n",
    "\n",
    "reset default matplotlib rcParams<br>\n",
    "`jtplot.reset()`\n",
    "\n",
    "Add table of contents and other add-ons<br>\n",
    "https://github.com/ipython-contrib/jupyter_contrib_nbextensions <br>\n",
    "`pip install jupyter_contrib_nbextensions`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter server\n",
    "\n",
    "__SSL - new key pem pair__<br>\n",
    "`openssl req -x509 -nodes -days 365 -newkey rsa:1024 -out cert.pem -keyout cert.key`<br>\n",
    "they are created in current folder (files cert.pem and cert.key) and need to be referenced in jupyter notebook config file<br>\n",
    "\n",
    "__Create password for Jupyter server__\n",
    "To create password for use with SSL and jupyter server<br> \n",
    "`ipython`<br>\n",
    "`import notebook.auth`<br>\n",
    "`type passwd`<br>\n",
    "You will be prompted for password. After you type in and hit return ipython will then give you in hash form.<br>\n",
    "\n",
    "__Jupyter Notebook Configuration File__\n",
    "VIM setup config file saved at `~/.jupyter/jupyter_notebook_config.py` on server and make sure contents as below:\n",
    "\n",
    "__SSL ENCRYPTION__<br>\n",
    "replace the following file names (and files used) by your choice/files<br>\n",
    "`c.NotebookApp.certfile = u'/root/.jupyter/cert.pem'`<br>\n",
    "`c.NotebookApp.keyfile = u'/root/.jupyter/cert.key'`<br>\n",
    "\n",
    "__IP ADDRESS AND PORT__<br>\n",
    "set ip to '*' to bind on all IP addresses of the cloud instance<br>\n",
    "`c.NotebookApp.ip = '*'`<br>\n",
    "it is a good idea to set a known, fixed default port for server access<br>\n",
    "`c.NotebookApp.port = 8888`\n",
    "\n",
    "__PASSWORD PROTECTION__<br>\n",
    "replace the hash code with the one for your password<br>\n",
    "`c.NotebookApp.password = HASH_CODE`<br>\n",
    "\n",
    "__NO BROWSER OPTION__\n",
    "prevent Jupyter from trying to open a browser\n",
    "`c.NotebookApp.open_browser = False`\n",
    " \n",
    "\n",
    "Then simply run jupyter notebook& (in background and close terminal)\n",
    "\n",
    "You should then be able to access it by the url:\n",
    "\n",
    "https://IP_ADDRESS:PORT_NUMBER<br>\n",
    "(Default 8888)    <br>\n",
    "(changing your ip address and port as necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Pycharm\n",
    "Set vimrc settings in pycharm<br>\n",
    "Copy `.vimrc` into `.ideavimrc` both in `~/` directory\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "### Web Scraping/Crawling \n",
    "\n",
    "#### Basic xpath syntax\n",
    "https://devhints.io/xpath<br>\n",
    "https://stackoverflow.com/questions/5033955/xpath-select-text-node<br>\n",
    "https://stackoverflow.com/questions/41075312/direct-text-contents-via-xpath<br>\n",
    "https://stackoverflow.com/questions/9493732/difference-between-text-and-string<br>\n",
    "\n",
    "`//` = search whole page<br>\n",
    "`/` = search for first instance<br>\n",
    "`[@class=”Class_1”]` = search for class named Class_1<br>\n",
    "\n",
    "\n",
    "#### Lxml Approach (see example scripts)\n",
    "\n",
    "__Get cookie__<br>\n",
    "1:  Can go into console in inspector and type document.cookie<br>\n",
    "2:  Can use request.get and then request.cookie as below<br>\n",
    "Request.get(url, proxies=proxies)<br>\n",
    "Cookie = request.cookie<br>\n",
    "Headers = Cookie<br>\n",
    "\n",
    "Note document.cookie will not include http only cookies (you can see these be looking in dev tools—application—cookies and seeing which ones have a tick in the http column.<br>\n",
    "\n",
    "In order to get past this you can just:<br>\n",
    "1.\tGo to the page inspector and then the network tab. (Can also go via Settings -> More tools -> Developer tools)\n",
    "2.\tSelect the network tab, and reload the page and sometimes may need to do few actions on the site.\n",
    "3.\tUnder Name, select the top item (usually the name of the page).\n",
    "4.\tUnder the Headers tab, find cookie: under Request Headers. The cookie is the text following cookie:\n",
    "5.\tCreate a dictionary in your python file as follows:<br>\n",
    "`headers = `{<br>\n",
    "`\t'Cookie': text`<br>\n",
    "`}`<br>\n",
    "where text is the cookie text you copied.<br>\n",
    "1.\tWhen requesting a page, amend the code in the following way:<br>\n",
    "\t`requests.get(recipe_url, headers=headers)`\n",
    "\n",
    "__Get desired data__<br>\n",
    "Right click on data and click inspect and then dive into particular node until get piece you need and then code below:\n",
    "\n",
    "__Get page to scrape__<br>\n",
    "`Request = request.get(url, proxies=proxies, headers=headers)`<br>\n",
    "`Page= html.fromstring(request.content)`<br>\n",
    "\n",
    "__Scrape page for data__<br>\n",
    "Get full table (usually table/tbody/tr)<br>\n",
    "`Rows = page.xpath(‘//table/tbody/tr’)`<br>\n",
    "Iterate over table and get data in each row<br>\n",
    " `'./td/text()'`\n",
    "\n",
    "Need to use get first routine to remove list structure<br>\n",
    "`def get_first(container):`<br>\n",
    "`    return container[1] if container else None`\n",
    "\n",
    "#### Scrapy\n",
    "#### Create new project\n",
    "`scrapy startproject PROJECT_NAME`\n",
    "\n",
    "__Run spider from inside project__<br>\n",
    "`Scrapy crawl SPIDER_NAME`\n",
    "\n",
    "See example code for structure but essentially below is example of what each file should contain:<br>\n",
    "\n",
    "__main_scraper_file.py__ (Contains spiders and scraping logic)\n",
    "\n",
    "__Start_requests__<br>\n",
    "List yield functions here.<br>\n",
    "( Note if callback function stated in scrapy.request then scrapy will cache bunches of pages and scrape them in random order which if you want it to stop after certain number of pages proves tricky as for example might go ahead and scrape page 290 before 286 even though you wanted it to stop at 285.)<br>\n",
    "https://stackoverflow.com/questions/16875580/the-order-of-scrapy-crawling-urls-with-long-start-urls-list-and-urls-yiels-from\n",
    "\n",
    "\n",
    "__Parse__<br>\n",
    "Always parse function present as initial parse of pages\n",
    "\n",
    "__Second_layer_parse__<br>\n",
    "Can then parse the output from first parse for example to crawl all the individual links on each page\n",
    "\n",
    "__Items.py file__<br>\n",
    "Define any items dictionaries you need here. You cannot collect items without first defining the item structure in here. See example below<br>\n",
    "\n",
    "`class ExampleItem(scrapy.Item):`<br>\n",
    "    `# define the fields for your item here like:`<br>\n",
    "     `name = scrapy.Field()`<br>\n",
    "     `date = scrapy.Field()`<br>\n",
    "     `court = scrapy.Field()`<br>\n",
    "     `exchange = scrapy.Field()`<br>\n",
    "     `ticker = scrapy.Field()`<br>\n",
    "     `url = scrapy.Field()`<br>\n",
    "\n",
    "\n",
    "__scrapy shell commands (shell useful for testing xpaths and debugging)__<br>\n",
    "\n",
    "__Grab desired page__<br>\n",
    "`fetch('SITE_URL')`\n",
    "\n",
    "\n",
    "__Add User-Agent:__<br>\n",
    "First note User-Agent can be found in same place as cookie<br>\n",
    "In order to get past this you can just:<br>\n",
    "1.\tGo to Settings -> More tools -> Developer tools\n",
    "2.\tSelect the network tab, and reload the page and sometimes may need to do few actions on the site.\n",
    "3.\tUnder Name, select the top item (usually the name of the page).<br>\n",
    "Under the Headers tab, find User-Agent: under Request Headers. The User-Agent text is usually after cookie header and looks like:<br>\n",
    "`Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36`<br>\n",
    "\n",
    "Sometimes websites need extra headers such as user_agent or cookie. Usually Just user-agent solves this problem by entering the below commands in scrapy shell<br>\n",
    "\n",
    "`from scrapy import Request`<br>\n",
    "`req = Request('SITE_URL', headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'})`<br>\n",
    "`fetch(req)`<br>\n",
    "\n",
    "if needed can also add cookie with:<br>\n",
    "`from scrapy import Request`<br>\n",
    "\n",
    "`req = Request('SITE_URL', headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36', ‘Cookie’: ‘ENTER_COOKIE’})`<br>\n",
    "`fetch(req)`<br>\n",
    "\n",
    "__Extract particular xpath, (use extract_first() to get first element if more than one)__<br>\n",
    "`response.xpath('//*[@id=\"records\"]/div[2]/ul/li[7]/@class').extract()`\n",
    "\n",
    "__To speed up spider try__<br>\n",
    "https://stackoverflow.com/questions/17029752/speed-up-web-scraper)<br>\n",
    "\n",
    "•\tuse latest scrapy version (if not using already)<br>\n",
    "•\tcheck if non-standard middlewares are used<br>\n",
    "•\ttry to increase CONCURRENT_REQUESTS_PER_DOMAIN, CONCURRENT_REQUESTS settings (docs)<br>\n",
    "•\tturn off logging LOG_ENABLED = False (docs)<br>\n",
    "•\ttry yielding an item in a loop instead of collecting items into the items list and returning them<br>\n",
    "•\tuse local cache DNS (see this thread)<br>\n",
    "•\tcheck if this site is using download threshold and limits your download speed (see this thread)<br>\n",
    "•\tlog cpu and memory usage during the spider run - see if there are any problems there<br>\n",
    "•\ttry run the same spider under scrapyd service<br>\n",
    "•\tsee if grequests + lxml will perform better (ask if you need any help with implementing this solution)<br>\n",
    "•\ttry running Scrapy on pypy, see Running Scrapy on PyPy<br>\n",
    "\n",
    "\n",
    "__Scraping forms using selenium__\n",
    "https://www.vultr.com/docs/how-to-install-phantomjs-on-ubuntu-16-04\n",
    "http://phantomjs.org/download.html<br>\n",
    "\n",
    "__Install phantomJS to enable use of selenium webdriver without GUI__<br>\n",
    "Step 1: Update the system<br>\n",
    "Before starting, it is recommended to update the system with the latest stable release. You can do this with the following command:<br>\n",
    "`sudo apt-get update -y`<br>\n",
    "`sudo apt-get upgrade -y`<br>\n",
    "`sudo shutdown -r now`<br>\n",
    "__Step 2: Install PhantomJS__<br>\n",
    "Before installing PhantomJS, you will need to install some required packages on your system. You can install all of them with the following command:<br>\n",
    "`sudo apt-get install build-essential chrpath libssl-dev libxft-dev libfreetype6-dev libfreetype6 libfontconfig1-dev libfontconfig1 -y`<br>\n",
    "Next, you will need to download the PhantomJS. You can download the latest stable version of the PhantomJS from their official website. Run the following command to download PhantomJS:<br>\n",
    "`sudo wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2`<br>\n",
    "If above doesn’t work download the file from below link and copy tar file into directory<br>\n",
    "http://phantomjs.org/download.html<br>\n",
    "\n",
    "Once the download is complete, extract the downloaded archive file to desired system location:<br>\n",
    "`sudo tar xvjf phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/share/`<br>\n",
    "Next, create a symlink of PhantomJS binary file to systems bin dirctory:<br>\n",
    "`sudo ln -s /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin/`<br>\n",
    "Step 3: Verify PhantomJS<br>\n",
    "PhantomJS is now installed on your system. You can now verify the installed version of PhantomJS with the following command:<br>\n",
    "`phantomjs --version`<br>\n",
    "You should see the following output:<br>\n",
    "`2.1.1`<br>\n",
    "You can also find the version of the PhantomJS from PhantomJS prompt as shown below:<br>\n",
    "phantomjs<br>\n",
    "You will get the phantomjs prompt:<Br>\n",
    "`phantomjs>`<br>\n",
    "Now, run the following command to find the version details:<br>\n",
    "`phantomjs> phantom.version`<br>\n",
    "You should see the following output:<br>\n",
    "`{`<br>\n",
    "   `\"major\": 2,`<br>\n",
    "   `\"minor\": 1,`<br>\n",
    "   `\"patch\": 1`<br>\n",
    "`}`<br>\n",
    "That's it. You have successfully installed PhantomJS on Ubuntu 16.04 server<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting/Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "\n",
    "#### Useful Links\n",
    "https://regexr.com/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### String formatting\n",
    "\n",
    "See code cell below for example of using F string to create clean standardised output for variable length strings.<br>\n",
    "\n",
    "With f string simply use a colon inside the curly brackets to define formatting for example:\n",
    "f’{Variable_1:>15}#’ means Variable will be right justified with the total column width (including the variable length) of 15 chars. It then places the# at the end of the 15 spaces.\n",
    "ie    `“     Variable_1#”`\n",
    "\n",
    "f’{Variable_1:<15} means Variable will be left justified with the total column width (including the variable length) of 15 chars. It then places the # at the end of the 15 spaces.\n",
    "ie    `“Variable_1     #”`\n",
    "\n",
    "See example code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           TEST#\n",
      "TEST           #\n"
     ]
    }
   ],
   "source": [
    "variable_1 = 'TEST'\n",
    "print(f'{variable_1:>15}#')\n",
    "print(f'{variable_1:<15}#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numeric variables use decimal point to specify accuracy. See code below where second example specifies 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6000.000000%#\n",
      "6000.00%       #\n"
     ]
    }
   ],
   "source": [
    "variable_1 = 60\n",
    "print(f'{variable_1:>15%}#')\n",
    "print(f'{variable_1:<15.2%}#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic example where using aligning strings of variable length using the max length forn the string column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Julian     Australia\n",
      "2. Bob        Spain\n",
      "3. PyBites    Global\n",
      "4. Dante      Argentina\n",
      "5. Martin     USA\n",
      "6. Rodolfo    Mexico\n"
     ]
    }
   ],
   "source": [
    "names = 'Martin Niamh Paul'.split()\n",
    "countries = 'Australia Ireland France'.split()\n",
    "\n",
    "\n",
    "def enumerate_names_countries():\n",
    "    \"\"\"Outputs:\n",
    "       1. Martin     Australia\n",
    "       2. Niamh      Ireland\n",
    "       3. Paul       France\"\"\"\n",
    "    j=0\n",
    "    res=list()\n",
    "    max_len = len(max(names, key=len))\n",
    "    for c, value in enumerate(names, 1):\n",
    "        dis = 4+max_len-len(value)\n",
    "        # Line below inserts gap equal to max string length minus\n",
    "        #length of particual string and then right justifies countries to next space after this point\n",
    "        print(f'{c}. '+f'{value}'+' '*dis+f'{countries[j]:<}')\n",
    "        j+=1\n",
    "        \n",
    "enumerate_names_countries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prevent Setting WithCopyWarning in pandas\n",
    "Usually using .apply on the column in question instead of the function itself will solve this issue.so for datetime example using\n",
    "\n",
    "`df['ref_period_end'].apply(pd.to_datetime)`<br>\n",
    "Instead of:<br>\n",
    "`pd.to_datetime(df['date'])`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show max rows and columns inline for pandas dataframes\n",
    "setting all rows to be visible in IDE<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.html.table_schema = True\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in series of csv files and concatenate into one csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 files to join\n",
      "Already Joined\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "all_files = glob.glob(os.getcwd() + \"/example_datasets/stock_data/*stock_market_data*.csv*\")\n",
    "print(len(all_files), 'files to join')\n",
    "\n",
    "li = []\n",
    "for filename in all_files:\n",
    "    try: \n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        li.append(df)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "file = glob.glob(os.getcwd() + '/example_datasets/stock_data/master*.csv')\n",
    "if len(file)>0:\n",
    "    print('Already Joined')\n",
    "else:\n",
    "    frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "    print('joined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations\n",
    "\n",
    "### Graph Database Packages\n",
    "\n",
    "Useful Links<br>\n",
    "https://json-ld.github.io/normalization/spec/\n",
    "\n",
    "NOTE: GRAPH PACKAGES NEED JAVE JDK-8<br>\n",
    "\n",
    "#### Gephi (graph creator) and Yed (graph visualiser)\n",
    "\n",
    "##### Linux setup\n",
    "\n",
    "__Install gephi linux__<br>\n",
    "Download from link below<br>\n",
    "https://gephi.org/users/download/<br>\n",
    "Then unzip folder in linux<br>\n",
    "`unzip gephi-0.9.2-linux.gz -d /Desktop/gephi`<br>\n",
    "if tar and gzip done on file then<br>\n",
    "`tar xzvf gephi-0.9.2-linux.tar.gz`<br>\n",
    "Run it by executing `./bin/gephi` script file inside gephi folder<br>\n",
    "\n",
    "__Install yed for linux__<br>\n",
    "Download from link below<br>\n",
    "https://www.yworks.com/downloads#yEd<br>\n",
    "Run bash file<br>\n",
    "`yEd-3.18.1.1_with-JRE8_64-bit_setup.sh`<br>\n",
    "\n",
    "##### Windows setup<br>\n",
    "Install java on windows (needed for graph languages like gephi and grakn)<br>\n",
    "First download and install Java SE Runtime Environment 8 from <br>\n",
    "https://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html <br>\n",
    "File is called `jre-8u191-windows-x64` and should be install at `c:/program_files/java` <br>\n",
    "\n",
    "__Install gephi windows__<br>\n",
    "Download executable from link below<br>\n",
    "https://gephi.org/users/download/<br>\n",
    "\n",
    "__Change default folder for java__<br>\n",
    " this line should be in gephi.conf file in `C:\\Program Files\\Gephi-0.9.2\\etc`<br>\n",
    "default location of JDK/JRE, can be overridden by using `--jdkhome <dir> switch`<br>\n",
    "`jdkhome=\"C:\\Program Files\\Java\\jre1.8.0_161\"`<br>\n",
    "    \n",
    "__Install yed for windows__<br>\n",
    "Download from link below<br>\n",
    "https://www.yworks.com/downloads#yEd<br>\n",
    "\n",
    "__Run script to create graphml file__<br>\n",
    "python MAIN_PYTHON_SCRIPT(net.py) DATA_FILE(data.csv) PATH_TO_GRAPHML_FILE(out.graphml) VAR_1 VAR_2 COLOUR_HEX_VAR1(\"#ff0000\") COLOUR_HEX_VAR2(<\"#0000ff\")  EDGE_VAR1_VAR2 SIZE_CALC_VAR2<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ipywidgets to give interactive graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set graph themes and pandas view defaults\n",
    "#jtplot.reset()\n",
    "pd.options.display.html.table_schema = True\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.dataresource+json": {
       "data": [
        {
         "date": "2015-09-25T00:00:00.000Z",
         "index": 0,
         "price": 11.92,
         "symbol": "OWW",
         "volume": 0
        },
        {
         "date": "2015-09-24T00:00:00.000Z",
         "index": 1,
         "price": 11.92,
         "symbol": "OWW",
         "volume": 0
        },
        {
         "date": "2015-09-23T00:00:00.000Z",
         "index": 2,
         "price": 11.92,
         "symbol": "OWW",
         "volume": 0
        },
        {
         "date": "2015-09-22T00:00:00.000Z",
         "index": 3,
         "price": 11.92,
         "symbol": "OWW",
         "volume": 0
        },
        {
         "date": "2015-09-21T00:00:00.000Z",
         "index": 4,
         "price": 11.92,
         "symbol": "OWW",
         "volume": 0
        }
       ],
       "schema": {
        "fields": [
         {
          "name": "index",
          "type": "integer"
         },
         {
          "name": "date",
          "type": "datetime"
         },
         {
          "name": "price",
          "type": "number"
         },
         {
          "name": "symbol",
          "type": "string"
         },
         {
          "name": "volume",
          "type": "integer"
         }
        ],
        "pandas_version": "0.20.0",
        "primaryKey": [
         "index"
        ]
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>symbol</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-09-25</td>\n",
       "      <td>11.92</td>\n",
       "      <td>OWW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-09-24</td>\n",
       "      <td>11.92</td>\n",
       "      <td>OWW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-09-23</td>\n",
       "      <td>11.92</td>\n",
       "      <td>OWW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-09-22</td>\n",
       "      <td>11.92</td>\n",
       "      <td>OWW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-09-21</td>\n",
       "      <td>11.92</td>\n",
       "      <td>OWW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  price symbol  volume\n",
       "0 2015-09-25  11.92    OWW       0\n",
       "1 2015-09-24  11.92    OWW       0\n",
       "2 2015-09-23  11.92    OWW       0\n",
       "3 2015-09-22  11.92    OWW       0\n",
       "4 2015-09-21  11.92    OWW       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare the data\n",
    "\n",
    "#stock data\n",
    "stock_df = pd.read_csv(os.getcwd()+'/example_datasets/cyber_data/stock_market_data.csv')\n",
    "stock_df['date'] = pd.to_datetime(stock_df.date)\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data\n",
    "\n",
    "#cyber data\n",
    "cyber_df = pd.read_csv(os.getcwd()+'/example_datasets/cyber_data/cyber_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1', '2', '3', '4', '5', 'financial', nan, 'oops!', 'y'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(cyber_df['DATA SENSITIVITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyber_df['DATA SENSITIVITY'] = cyber_df['DATA SENSITIVITY'].replace({'financial':'4','oops!': '1', 'y':'2'  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyber_df['DATA SENSITIVITY'] = cyber_df['DATA SENSITIVITY'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(cyber_df['DATA SENSITIVITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyber_df['cyber_date'] = pd.to_datetime(\n",
    "    cyber_df['story'].apply(lambda x: x[:8]).str.upper(),\n",
    "    format='%b %Y',\n",
    "    yearfirst=False,errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyber_df['year'] = cyber_df['cyber_date'].apply(lambda x: x.year).fillna(0).astype(int)\n",
    "cyber_df['month'] = cyber_df['cyber_date'].apply(lambda x: x.month).fillna(0).astype(int)\n",
    "cyber_df['key'] = cyber_df['Ticker'].astype(str)+cyber_df['year'].astype(str)+cyber_df['month'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.dataresource+json": {
       "data": [
        {
         "1st source link": "http://uk.businessinsider.com/yahoo-hack-by-state-sponsored-actor-biggest-of-all-time-2016-9?r=US&IR=T",
         "2nd source link": null,
         "DATA SENSITIVITY": 2,
         "DISPLAYED RECORDS": "500,000,000",
         "Entity": "Yahoo",
         "METHOD": "hacked",
         "Market": "Nasdaq ",
         "SECTOR": "web",
         "Ticker": "YHOO",
         "Unnamed: 16": null,
         "Unnamed: 17": null,
         "YEAR": 2016,
         "alternative name": null,
         "column 11": null,
         "cyber_date": "2016-09-01T00:00:00.000Z",
         "index": 0,
         "interesting story": null,
         "key": "YHOO20169",
         "month": 9,
         "records lost": "500,000,000",
         "source name": "Business Insider",
         "story": "Sep 2016. Happened in 2014, but no. records stolen was originally thought to be much smaller. Yahoo revealed the real numbers in 2016.",
         "year": 2016
        },
        {
         "1st source link": "https://www.reuters.com/article/us-twitter-passwords/twitter-urges-all-users-to-change-passwords-after-glitch-idUSKBN1I42JG",
         "2nd source link": null,
         "DATA SENSITIVITY": 1,
         "DISPLAYED RECORDS": "330,000,000",
         "Entity": "Twitter",
         "METHOD": "poor security",
         "Market": "NYSE",
         "SECTOR": "app",
         "Ticker": "TWTR",
         "Unnamed: 16": null,
         "Unnamed: 17": null,
         "YEAR": 2018,
         "alternative name": null,
         "column 11": null,
         "cyber_date": "2018-05-01T00:00:00.000Z",
         "index": 1,
         "interesting story": null,
         "key": "TWTR20185",
         "month": 5,
         "records lost": "330,000,000",
         "source name": "Reuters",
         "story": "May 2018. A glitch caused some passwords to be stored in readable text, visible on the internal computer system.",
         "year": 2018
        },
        {
         "1st source link": "http://motherboard.vice.com/read/427-million-myspace-passwords-emails-data-breach",
         "2nd source link": null,
         "DATA SENSITIVITY": 1,
         "DISPLAYED RECORDS": "164,000,000",
         "Entity": "MySpace",
         "METHOD": "hacked",
         "Market": "NYSE",
         "SECTOR": "web",
         "Ticker": "NWS ",
         "Unnamed: 16": null,
         "Unnamed: 17": null,
         "YEAR": 2016,
         "alternative name": null,
         "column 11": null,
         "cyber_date": "2016-05-01T00:00:00.000Z",
         "index": 2,
         "interesting story": null,
         "key": "NWS 20165",
         "month": 5,
         "records lost": "164,000,000",
         "source name": "Motherboard",
         "story": "May 2016. The same hacker who was selling LinkedIn user data now claims to have MySpace user data too, and lots of it.",
         "year": 2016
        },
        {
         "1st source link": "https://www.theguardian.com/technology/2018/mar/30/hackers-steal-data-150m-myfitnesspal-app-users-under-armour",
         "2nd source link": null,
         "DATA SENSITIVITY": 1,
         "DISPLAYED RECORDS": "150,000,000",
         "Entity": "MyFitnessPal",
         "METHOD": "hacked",
         "Market": "NYSE",
         "SECTOR": "app",
         "Ticker": "UAA",
         "Unnamed: 16": null,
         "Unnamed: 17": null,
         "YEAR": 2018,
         "alternative name": "UnderArmour",
         "column 11": null,
         "cyber_date": "2018-03-01T00:00:00.000Z",
         "index": 3,
         "interesting story": null,
         "key": "UAA20183",
         "month": 3,
         "records lost": "150,000,000",
         "source name": "Guardian",
         "story": "Mar 2018. Usernames, email addresses, and hashed user passwords were stolen.",
         "year": 2018
        },
        {
         "1st source link": "https://www.businessinsider.com/cyber-thieves-took-data-on-145-million-ebay-customers-by-hacking-3-corporate-employees-2014-5?r=US&IR=T",
         "2nd source link": null,
         "DATA SENSITIVITY": 1,
         "DISPLAYED RECORDS": "145,000,000",
         "Entity": "Ebay",
         "METHOD": "hacked",
         "Market": "NASDAQ",
         "SECTOR": "web",
         "Ticker": "EBAY",
         "Unnamed: 16": null,
         "Unnamed: 17": null,
         "YEAR": 2014,
         "alternative name": null,
         "column 11": null,
         "cyber_date": "2014-05-01T00:00:00.000Z",
         "index": 4,
         "interesting story": "y",
         "key": "EBAY20145",
         "month": 5,
         "records lost": "145,000,000",
         "source name": "Business Insider",
         "story": "May 2014. The company has said hackers attacked between late February and early March with login credentials obtained from â€œa small numberâ€ of employees. They then accessed a database containing all user records and copied â€œa large partâ€ of those credentials.",
         "year": 2014
        }
       ],
       "schema": {
        "fields": [
         {
          "name": "index",
          "type": "integer"
         },
         {
          "name": "Entity",
          "type": "string"
         },
         {
          "name": "Ticker",
          "type": "string"
         },
         {
          "name": "Market",
          "type": "string"
         },
         {
          "name": "alternative name",
          "type": "string"
         },
         {
          "name": "records lost",
          "type": "string"
         },
         {
          "name": "YEAR",
          "type": "integer"
         },
         {
          "name": "story",
          "type": "string"
         },
         {
          "name": "SECTOR",
          "type": "string"
         },
         {
          "name": "METHOD",
          "type": "string"
         },
         {
          "name": "interesting story",
          "type": "string"
         },
         {
          "name": "DATA SENSITIVITY",
          "type": "integer"
         },
         {
          "name": "DISPLAYED RECORDS",
          "type": "string"
         },
         {
          "name": "column 11",
          "type": "number"
         },
         {
          "name": "source name",
          "type": "string"
         },
         {
          "name": "1st source link",
          "type": "string"
         },
         {
          "name": "2nd source link",
          "type": "string"
         },
         {
          "name": "Unnamed: 16",
          "type": "string"
         },
         {
          "name": "Unnamed: 17",
          "type": "string"
         },
         {
          "name": "cyber_date",
          "type": "datetime"
         },
         {
          "name": "year",
          "type": "integer"
         },
         {
          "name": "month",
          "type": "integer"
         },
         {
          "name": "key",
          "type": "string"
         }
        ],
        "pandas_version": "0.20.0",
        "primaryKey": [
         "index"
        ]
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Market</th>\n",
       "      <th>alternative name</th>\n",
       "      <th>records lost</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>story</th>\n",
       "      <th>SECTOR</th>\n",
       "      <th>METHOD</th>\n",
       "      <th>interesting story</th>\n",
       "      <th>DATA SENSITIVITY</th>\n",
       "      <th>DISPLAYED RECORDS</th>\n",
       "      <th>column 11</th>\n",
       "      <th>source name</th>\n",
       "      <th>1st source link</th>\n",
       "      <th>2nd source link</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>cyber_date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yahoo</td>\n",
       "      <td>YHOO</td>\n",
       "      <td>Nasdaq</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500,000,000</td>\n",
       "      <td>2016</td>\n",
       "      <td>Sep 2016. Happened in 2014, but no. records st...</td>\n",
       "      <td>web</td>\n",
       "      <td>hacked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>500,000,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>http://uk.businessinsider.com/yahoo-hack-by-st...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>YHOO20169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>TWTR</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>330,000,000</td>\n",
       "      <td>2018</td>\n",
       "      <td>May 2018. A glitch caused some passwords to be...</td>\n",
       "      <td>app</td>\n",
       "      <td>poor security</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>330,000,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>https://www.reuters.com/article/us-twitter-pas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>TWTR20185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MySpace</td>\n",
       "      <td>NWS</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164,000,000</td>\n",
       "      <td>2016</td>\n",
       "      <td>May 2016. The same hacker who was selling Link...</td>\n",
       "      <td>web</td>\n",
       "      <td>hacked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>164,000,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Motherboard</td>\n",
       "      <td>http://motherboard.vice.com/read/427-million-m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>NWS 20165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MyFitnessPal</td>\n",
       "      <td>UAA</td>\n",
       "      <td>NYSE</td>\n",
       "      <td>UnderArmour</td>\n",
       "      <td>150,000,000</td>\n",
       "      <td>2018</td>\n",
       "      <td>Mar 2018. Usernames, email addresses, and hash...</td>\n",
       "      <td>app</td>\n",
       "      <td>hacked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>150,000,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Guardian</td>\n",
       "      <td>https://www.theguardian.com/technology/2018/ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>UAA20183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ebay</td>\n",
       "      <td>EBAY</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145,000,000</td>\n",
       "      <td>2014</td>\n",
       "      <td>May 2014. The company has said hackers attacke...</td>\n",
       "      <td>web</td>\n",
       "      <td>hacked</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>145,000,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>https://www.businessinsider.com/cyber-thieves-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-05-01</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>EBAY20145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Entity Ticker   Market alternative name records lost  YEAR  \\\n",
       "0         Yahoo   YHOO  Nasdaq               NaN  500,000,000  2016   \n",
       "1       Twitter   TWTR     NYSE              NaN  330,000,000  2018   \n",
       "2       MySpace   NWS      NYSE              NaN  164,000,000  2016   \n",
       "3  MyFitnessPal    UAA     NYSE      UnderArmour  150,000,000  2018   \n",
       "4          Ebay   EBAY   NASDAQ              NaN  145,000,000  2014   \n",
       "\n",
       "                                               story SECTOR         METHOD  \\\n",
       "0  Sep 2016. Happened in 2014, but no. records st...    web         hacked   \n",
       "1  May 2018. A glitch caused some passwords to be...    app  poor security   \n",
       "2  May 2016. The same hacker who was selling Link...    web         hacked   \n",
       "3  Mar 2018. Usernames, email addresses, and hash...    app         hacked   \n",
       "4  May 2014. The company has said hackers attacke...    web         hacked   \n",
       "\n",
       "  interesting story  DATA SENSITIVITY DISPLAYED RECORDS  column 11  \\\n",
       "0               NaN                 2       500,000,000        NaN   \n",
       "1               NaN                 1       330,000,000        NaN   \n",
       "2               NaN                 1       164,000,000        NaN   \n",
       "3               NaN                 1       150,000,000        NaN   \n",
       "4                 y                 1       145,000,000        NaN   \n",
       "\n",
       "        source name                                    1st source link  \\\n",
       "0  Business Insider  http://uk.businessinsider.com/yahoo-hack-by-st...   \n",
       "1           Reuters  https://www.reuters.com/article/us-twitter-pas...   \n",
       "2       Motherboard  http://motherboard.vice.com/read/427-million-m...   \n",
       "3          Guardian  https://www.theguardian.com/technology/2018/ma...   \n",
       "4  Business Insider  https://www.businessinsider.com/cyber-thieves-...   \n",
       "\n",
       "  2nd source link Unnamed: 16 Unnamed: 17 cyber_date  year  month        key  \n",
       "0             NaN         NaN         NaN 2016-09-01  2016      9  YHOO20169  \n",
       "1             NaN         NaN         NaN 2018-05-01  2018      5  TWTR20185  \n",
       "2             NaN         NaN         NaN 2016-05-01  2016      5  NWS 20165  \n",
       "3             NaN         NaN         NaN 2018-03-01  2018      3   UAA20183  \n",
       "4             NaN         NaN         NaN 2014-05-01  2014      5  EBAY20145  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyber_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.dataresource+json": {
       "data": [
        {
         "1st source link": "http://money.cnn.com/2016/05/19/technology/linkedin-hack/",
         "2nd source link": "https://money.cnn.com/2012/06/06/technology/linkedin-password-hack/?iid=EL",
         "DATA SENSITIVITY": 1,
         "DISPLAYED RECORDS": "117,000,000",
         "Entity": "LinkedIn",
         "METHOD": "hacked",
         "Market": null,
         "SECTOR": "web",
         "Ticker": "LNKD",
         "Unnamed: 16": null,
         "Unnamed: 17": null,
         "YEAR": 2016,
         "alternative name": null,
         "column 11": null,
         "cyber_date": "2016-05-01T00:00:00.000Z",
         "index": 7,
         "interesting story": null,
         "key": "LNKD20165",
         "month": 5,
         "records lost": "117,000,000",
         "source name": "CNN",
         "story": "May 2016. What initially seemed to be a theft of 6.5 million passwords has actually turned out to be a breach of 117 million passwords.",
         "year": 2016
        },
        {
         "1st source link": "https://www.neowin.net/news/microsoft-owned-linkedin-is-sending-emails-to-users-about-a-lyndacom-data-breach",
         "2nd source link": null,
         "DATA SENSITIVITY": 1,
         "DISPLAYED RECORDS": null,
         "Entity": "Lynda.com",
         "METHOD": "hacked",
         "Market": null,
         "SECTOR": "web",
         "Ticker": "LNKD",
         "Unnamed: 16": null,
         "Unnamed: 17": null,
         "YEAR": 2016,
         "alternative name": "owned by LinkedIn",
         "column 11": null,
         "cyber_date": "2016-12-01T00:00:00.000Z",
         "index": 32,
         "interesting story": null,
         "key": "LNKD201612",
         "month": 12,
         "records lost": "9,500,000",
         "source name": "Neowin",
         "story": "Dec 2016. Hackers breached a database that held records of contact info and courses viewed. No official statement yet on how many records were actually stolen, and no evidence yet of them having been published anywhere.",
         "year": 2016
        },
        {
         "1st source link": "http://news.cnet.com/8301-1009_3-57449325-83/what-the-password-leaks-mean-to-you-faq/?tag=mncol;txt",
         "2nd source link": null,
         "DATA SENSITIVITY": 1,
         "DISPLAYED RECORDS": null,
         "Entity": "LinkedIn, eHarmony, Last.fm",
         "METHOD": "hacked",
         "Market": null,
         "SECTOR": "web",
         "Ticker": "LNKD",
         "Unnamed: 16": null,
         "Unnamed: 17": null,
         "YEAR": 2012,
         "alternative name": null,
         "column 11": null,
         "cyber_date": "2012-06-01T00:00:00.000Z",
         "index": 34,
         "interesting story": null,
         "key": "LNKD20126",
         "month": 6,
         "records lost": "8,000,000",
         "source name": "Cnet",
         "story": "Jun 2012. Hacker 'dwdm' uploaded a file containing 6.5 million passwords on a Russian hacker forum. Soon after another 1.5 million passwords were discovered.  On analysis, 93% of the passwords could be found in the Top 10,000 password list.",
         "year": 2012
        }
       ],
       "schema": {
        "fields": [
         {
          "name": "index",
          "type": "integer"
         },
         {
          "name": "Entity",
          "type": "string"
         },
         {
          "name": "Ticker",
          "type": "string"
         },
         {
          "name": "Market",
          "type": "string"
         },
         {
          "name": "alternative name",
          "type": "string"
         },
         {
          "name": "records lost",
          "type": "string"
         },
         {
          "name": "YEAR",
          "type": "integer"
         },
         {
          "name": "story",
          "type": "string"
         },
         {
          "name": "SECTOR",
          "type": "string"
         },
         {
          "name": "METHOD",
          "type": "string"
         },
         {
          "name": "interesting story",
          "type": "string"
         },
         {
          "name": "DATA SENSITIVITY",
          "type": "integer"
         },
         {
          "name": "DISPLAYED RECORDS",
          "type": "string"
         },
         {
          "name": "column 11",
          "type": "number"
         },
         {
          "name": "source name",
          "type": "string"
         },
         {
          "name": "1st source link",
          "type": "string"
         },
         {
          "name": "2nd source link",
          "type": "string"
         },
         {
          "name": "Unnamed: 16",
          "type": "string"
         },
         {
          "name": "Unnamed: 17",
          "type": "string"
         },
         {
          "name": "cyber_date",
          "type": "datetime"
         },
         {
          "name": "year",
          "type": "integer"
         },
         {
          "name": "month",
          "type": "integer"
         },
         {
          "name": "key",
          "type": "string"
         }
        ],
        "pandas_version": "0.20.0",
        "primaryKey": [
         "index"
        ]
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Market</th>\n",
       "      <th>alternative name</th>\n",
       "      <th>records lost</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>story</th>\n",
       "      <th>SECTOR</th>\n",
       "      <th>METHOD</th>\n",
       "      <th>interesting story</th>\n",
       "      <th>DATA SENSITIVITY</th>\n",
       "      <th>DISPLAYED RECORDS</th>\n",
       "      <th>column 11</th>\n",
       "      <th>source name</th>\n",
       "      <th>1st source link</th>\n",
       "      <th>2nd source link</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>cyber_date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>LNKD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117,000,000</td>\n",
       "      <td>2016</td>\n",
       "      <td>May 2016. What initially seemed to be a theft ...</td>\n",
       "      <td>web</td>\n",
       "      <td>hacked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>117,000,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CNN</td>\n",
       "      <td>http://money.cnn.com/2016/05/19/technology/lin...</td>\n",
       "      <td>https://money.cnn.com/2012/06/06/technology/li...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>LNKD20165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Lynda.com</td>\n",
       "      <td>LNKD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>owned by LinkedIn</td>\n",
       "      <td>9,500,000</td>\n",
       "      <td>2016</td>\n",
       "      <td>Dec 2016. Hackers breached a database that hel...</td>\n",
       "      <td>web</td>\n",
       "      <td>hacked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neowin</td>\n",
       "      <td>https://www.neowin.net/news/microsoft-owned-li...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>LNKD201612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LinkedIn, eHarmony, Last.fm</td>\n",
       "      <td>LNKD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8,000,000</td>\n",
       "      <td>2012</td>\n",
       "      <td>Jun 2012. Hacker 'dwdm' uploaded a file contai...</td>\n",
       "      <td>web</td>\n",
       "      <td>hacked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cnet</td>\n",
       "      <td>http://news.cnet.com/8301-1009_3-57449325-83/w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>2012</td>\n",
       "      <td>6</td>\n",
       "      <td>LNKD20126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Entity Ticker Market   alternative name records lost  \\\n",
       "7                      LinkedIn   LNKD    NaN                NaN  117,000,000   \n",
       "32                    Lynda.com   LNKD    NaN  owned by LinkedIn    9,500,000   \n",
       "34  LinkedIn, eHarmony, Last.fm   LNKD    NaN                NaN    8,000,000   \n",
       "\n",
       "    YEAR                                              story SECTOR  METHOD  \\\n",
       "7   2016  May 2016. What initially seemed to be a theft ...    web  hacked   \n",
       "32  2016  Dec 2016. Hackers breached a database that hel...    web  hacked   \n",
       "34  2012  Jun 2012. Hacker 'dwdm' uploaded a file contai...    web  hacked   \n",
       "\n",
       "   interesting story  DATA SENSITIVITY DISPLAYED RECORDS  column 11  \\\n",
       "7                NaN                 1       117,000,000        NaN   \n",
       "32               NaN                 1               NaN        NaN   \n",
       "34               NaN                 1               NaN        NaN   \n",
       "\n",
       "   source name                                    1st source link  \\\n",
       "7          CNN  http://money.cnn.com/2016/05/19/technology/lin...   \n",
       "32      Neowin  https://www.neowin.net/news/microsoft-owned-li...   \n",
       "34        Cnet  http://news.cnet.com/8301-1009_3-57449325-83/w...   \n",
       "\n",
       "                                      2nd source link Unnamed: 16 Unnamed: 17  \\\n",
       "7   https://money.cnn.com/2012/06/06/technology/li...         NaN         NaN   \n",
       "32                                                NaN         NaN         NaN   \n",
       "34                                                NaN         NaN         NaN   \n",
       "\n",
       "   cyber_date  year  month         key  \n",
       "7  2016-05-01  2016      5   LNKD20165  \n",
       "32 2016-12-01  2016     12  LNKD201612  \n",
       "34 2012-06-01  2012      6   LNKD20126  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyber_df[cyber_df['Ticker']==\"LNKD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that when combined with ipywidgets creats a dynamic visual with choices over desired parameters\n",
    "\n",
    "def plot_new(ticker='YHOO', price_metric='price', volume_metric='vol', no_axis=2):\n",
    "    pyplot.show()\n",
    "    fig = plt.figure(figsize=(15,9))\n",
    "    data = stock_df[stock_df['symbol'] == ticker]\n",
    "    data = data.sort_values(by=['date'], ascending=True)\n",
    "    data.index = pd.DatetimeIndex(data['date'], name='date_')\n",
    "    data['vol_sum'] = data['volume'].cumsum()\n",
    "    data = data.resample('M').first()\n",
    "    data['returns'] = data['price']/data['price'].shift(1)\n",
    "    data['vol'] = data['vol_sum'] - data['vol_sum'].shift(1)\n",
    "    data['% change'] = (data['vol']/data['vol'].shift(1))\n",
    "    data['year'] = data['date'].apply(lambda x: x.year)\n",
    "    data['month'] = data['date'].apply(lambda x: x.month)\n",
    "    data['d_key'] = data['symbol'].astype(str)+data['year'].astype(str)+data['month'].astype(str)\n",
    "    d = data.merge(cyber_df, left_on=data.d_key, right_on=cyber_df.key, how='left')\n",
    "    d = d.fillna(0)\n",
    "    d['data breach'] = np.where(d['Entity']!=0,d[price_metric],-100)\n",
    "    d = d[['date', 'price', 'returns', 'symbol', 'vol', '% change', 'data breach', 'DATA SENSITIVITY']]\n",
    "    d = d.iloc[2:]\n",
    "    if(no_axis==1):\n",
    "        #plot for single y axis\n",
    "        plt.plot(d['date'], d[price_metric])\n",
    "        plt.scatter(d['date'].tolist(), \n",
    "                    d['data breach'], c=d['DATA SENSITIVITY'], s=d['DATA SENSITIVITY']*100, cmap = 'Reds')\n",
    "        plt.ylim((0,d[price_metric].max()))\n",
    "        leg = plt.legend()\n",
    "        plt.xlabel('date')\n",
    "        plt.ylabel('stock price')\n",
    "        plt.colorbar()\n",
    "    else:\n",
    "        #plot for double y axis\n",
    "        ax=fig.add_subplot(111)\n",
    "        ax2 = ax.twinx()\n",
    "        ax.plot(d['date'], d[price_metric], linewidth=3.0)\n",
    "        ax2.plot(d['date'], d[volume_metric], 'g-', alpha=0.3, linewidth=2.0)\n",
    "        ax.scatter(d['date'].tolist(), d['data breach'],c='Red', s=d['DATA SENSITIVITY']*100)\n",
    "        ax.set_xlabel('date', fontsize=25)\n",
    "        ax.set_ylabel(price_metric, color='b',fontsize=25)\n",
    "        ax2.set_ylabel(volume_metric, color='g',fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5dc8556a344b039b90e345e1086075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='ticker', index=50, options=('PNRA', 'BANR', 'TER', 'DGX', 'UAA', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(plot_new, \n",
    "         ticker=list(set(stock_df['symbol'])), \n",
    "         price_metric=['price', 'returns'], \n",
    "         volume_metric=['vol','% change'],\n",
    "         no_axis=[1,2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Productionisation of code\n",
    "\n",
    "\n",
    "### python pip package method with GitHub\n",
    "\n",
    "#### Create pip package\n",
    "\n",
    "Follow instructions at https://medium.com/@thucnc/how-to-publish-your-own-python-package-to-pypi-4318868210f9<br>\n",
    "Also useful<br>\n",
    "https://dzone.com/articles/executable-package-pip-install<br>\n",
    "https://packaging.python.org/tutorials/packaging-projects/<br>\n",
    "\n",
    "\n",
    "1) Refactor code to publishable standard (PEP8 etc) remove all code outside of class, put in __main__ if necessary.<br>\n",
    "\n",
    "2) Create a python package. This folder contains files (modules) and other sub-folders (sub-packages).<br>\n",
    "And you need to put a file __init__.py (two underscores before and after init) to mark this folder as a Python package. Inside this __init__.py file you can specify which classes you want the user to access through the package interface.<br>\n",
    "Sample __init.py__ file (taken from my example repo).<br>\n",
    "\n",
    "`from . import sharetool`<br>\n",
    "<br>\n",
    "`__all__=[`<br>\n",
    "`       'sharetool'`<br>\n",
    "`       ]`<br>\n",
    "\n",
    "3) Create setup.py file which details the package (see example below)<br>\n",
    "\n",
    "`from setuptools import setup, find_packages`<br>\n",
    "<br>\n",
    "`with open(\"README.md\", \"r\") as readme_file:`<br>\n",
    "`    README = readme_file.read()`<br>\n",
    "<br>\n",
    "`setup_args = dict(`<br>\n",
    "`     name='PACKAGE_NAME',`<br>  \n",
    "`     version='VERSION_NUMBER',`<br>\n",
    "`     description=\"SENTENCE DESCRIBING PACKAGE\",`<br>\n",
    "`     long_description_content_type=\"text/markdown\",`<br>\n",
    "`     long_description= DETAILED README DESCRIBING PACKAGE,`<br>\n",
    "`     licence='LICENCE eg MIT',`<br>\n",
    "`     packages=find_packages(),`<br>\n",
    "`     author=\"AUTHOR NAME\",`<br>\n",
    "`     author_email=\"AUTHOR EMAIL\",`<br>\n",
    "`     keywords=[LIST OF KEYWORDS ASSOCIATED WITH PACKAGE],`<br>\n",
    "`     url=\"GITHUB URL LINKED TO PACKAGE\",`<br>\n",
    "`     download_url = PYPI URL OF PACKAGE`<br>\n",
    "\n",
    "<br>\n",
    "`install_requires = [LIST OF DEPENDENT PACKAGES]`<br>\n",
    "<br>\n",
    "`if __name__ == '__main__':`<br>\n",
    "`    setup(**setup_args, install_requires=install_requires)`<br>\n",
    "\n",
    "4) Register at https://pypi.org/account/register/.\n",
    "\n",
    "5) Generate distribution archives and upload to PyPi<br>\n",
    "\n",
    "Make sure you have the latest versions of setuptools and wheel installed:<br>\n",
    "`pip install --user --upgrade setuptools wheel`<br>\n",
    "\n",
    "Now run this command from the same directory where setup.py is located:<br>\n",
    "`python3 setup.py sdist bdist_wheel`<br>\n",
    "\n",
    "You should now have the below file tree:\n",
    "\n",
    "<img src=\"media/pip_1.png\">\n",
    "\n",
    "You should add all of these three folders to your .gitignore file.\n",
    "\n",
    "\n",
    "6) Uploading the distribution archives<br>\n",
    "To do this, you can use twine. First, install it using pip:<br>\n",
    "`pip install --user --upgrade twine`<br>\n",
    "Then upload all the archives to PyPi:<br>\n",
    "`twine upload dist/*`<br>\n",
    "... enter your PyPi username and password<br>\n",
    "After successful uploading, go to PyPi website, under your project, you can found your published package.<br>\n",
    "\n",
    "\n",
    "### python pip package method with Docker, TeamCity and Artifactory\n",
    "\n",
    "#### Useful links<br>\n",
    "\n",
    "This is alternative to uploading to pypi whereby you are uploading to your own artifactory repo say for a company. \n",
    "\n",
    "1) Dockerise your code using a template docker you have used before or setting up a new one with the necessary setup.py file and this file will be exactly the same as the above example. (if have repo on bitbucket for a previous package then could git clone this and create a new repo for new code as this will have docker etc already setup).\n",
    "\n",
    "2) Next upload to bitbucket repo or whatever repo you use to store code.\n",
    "\n",
    "3) Link this repo to TeamCity for continuous integration/continuous deployment (CI/CD).\n",
    "\n",
    "Teamcity instructions:<br>\n",
    "Go to Administration top right of screen then choose PROJECT<br>\n",
    "Then choose create subproject<br>\n",
    "Link to bitbucket repository, pasting in the clone link into repo URL<br>\n",
    "Enter project name<br>\n",
    "Enter build config name as Build<br>\n",
    "Do not auto-detect build steps<br>\n",
    "Instead go to parameters and attach to template<br>\n",
    "Choose docker Compose template and click associate<br>\n",
    "\n",
    "Now once inside your build go down each of the sections listed on the right hand side as below one by one:<br>\n",
    "\n",
    "__General settings__<br>\n",
    "Leave as default<br>\n",
    "\n",
    "__Version Control__ \n",
    "Leave as default<br>\n",
    "\n",
    "__Build__\n",
    "Only need command line build step<ve>\n",
    "\n",
    "inside the custom script dialog box insert below code\n",
    "\n",
    "These lines push build counter environment variable to be used by following step (running setup.py)<br>\n",
    "`export version_num=5.%env.BUILD_COUNTER%`<br>\n",
    "`echo \"##teamcity[setParameter name='env.version_num' value='5.%env.BUILD_COUNTER%']\"`<br>\n",
    "\n",
    "These lines run the setup.py script and push the package to artifactory<br>\n",
    "python3 setup.py sdist<br>\n",
    "python3 -m pip install --user --upgrade twine<br>\n",
    "python3 -m twine upload --repository-url https://LOCAL_URL_OF_REPO -u teamcity -p PASSWORD! --verbose dist/*<br>\n",
    "\n",
    "\n",
    "__IMPORTANT NOTE – UPDATE SETUP.PY TO GET VERSION NUMBER__<br>\n",
    "Also remember to add following lines to setup.py. <br>\n",
    "\n",
    "try:<br>\n",
    "     version = os.environ['version_num']<br>\n",
    "except:<br>\n",
    "     version = pkg_resources.require('alphavantage_data_collector')[0].version<br>\n",
    "\n",
    "The __try__ line is used first time __setup.py__ is run as the package is created and gives it the current build number.<br>\n",
    "The __except__ line then pulls the version number from inside the package when setup.py is run again locally when installing the package. This ensures that the package you install locally will have same version number as the teamcity package.<br>\n",
    "Essentially when it is being built on teamcity it can access local environment variables on teamcity (the try line). However when it is downloaded it is on your local machine and can no longer access this teamcity variable and as such now needs to pull the version number from the downloaded package (the except line). <br>\n",
    "\n",
    "__The below lines explain it well__ <br>\n",
    "https://stackoverflow.com/questions/2058802/how-can-i-get-the-version-defined-in-setup-py-setuptools-in-my-package<br>\n",
    "https://stackoverflow.com/questions/8219493/teamcity-passing-an-id-generated-in-one-build-step-to-a-later-build-step<br>\n",
    "\n",
    "__Additional links__<br>\n",
    "https://stackoverflow.com/questions/30154647/access-teamcity-environment-variables-with-python-script<br>\n",
    "https://stackoverflow.com/questions/20829161/teamcity-using-setparameter-to-pass-information-between-build-steps/27606166<br>\n",
    "https://teamcity-support.jetbrains.com/hc/en-us/community/posts/206865865-Access-Build-Counter-as-an-environment-variable<br>\n",
    "https://github.com/jonnyzzz/TeamCity.Virtual/issues/40<br>\n",
    "https://stackoverflow.com/questions/2058802/how-can-i-get-the-version-defined-in-setup-py-setuptools-in-my-package<br>\n",
    "\n",
    "__Artifactory server URL:__<br>\n",
    "REPO_URL:build-LATEST<br>\n",
    "    \n",
    "__Triggers__<br>\n",
    "(ensure trigger is in place for each VCS check-in) (Note as only need package step this will just push new package to artifactory and not rebuild docker etc)<br>\n",
    "\n",
    "__Parameters__<br>\n",
    "(ensure BUILD_COUNTER is defined here as it is used in the custom script in the build step)<br>\n",
    "\n",
    "__Agent Requirements__<br>\n",
    "Assign the appropriate build agent to the project.<br>\n",
    "\n",
    "\n",
    "### Example usage (stock_data_collector)\n",
    "\n",
    "##### INSTALLATION\n",
    "`pip3 install alphavantage_data_collector` <br>\n",
    "\n",
    "##### IMPORT PACKAGE\n",
    "`from alphavantage_data_collector import sharetool`<br>\n",
    "\n",
    "##### Initialise instance of class\n",
    "First initialise the instance with the user API key<br>\n",
    "`al = sharetool.alpha(user_api='API_CODE')`<br>\n",
    "Next two options, either:<br>\n",
    "\n",
    "##### OPTION A:<br> \n",
    "Feed in list of tickers<br>\n",
    "`al.alpha_collector(tickers=['AAPL'])`<br>\n",
    "\n",
    "##### OPTION B:<br>\n",
    "Feed in csv file named ticker_input.csv with column named ‘tickers’ (useful if have large dataset and want to augment existing data with stock price e.g. add stock price to Marine clients)<br>\n",
    "Input is given as ticker_input.csv and package takes list of tickers from a column named ‘tickers’ in this file, all other data in input file is ignored. Simple run alpha_collector with no input for the tickers parameter.<br> \n",
    "`al.alpha_collector()`<br>\n",
    "\n",
    "##### Combine output files into master file\n",
    "Finally run the combine data script to collate any output files from separate runs into one master file.<br>\n",
    "`al.combine_data()`\n",
    "\n",
    "##### OUTPUT\n",
    "creates 3 files:<br>\n",
    "`completed_tickers.csv` = list of tickers collected from input list <br>\n",
    "`missing_tickers.csv` = list of tickers not located from input list <br>\n",
    "`master_stock_data_timestampXXXX.csv` = tick data output with timestamp for easy indexing<br>\n",
    "\n",
    "\n",
    "### Deploy package from Cloudera to artifactory\n",
    "Simple click on set me up on top right of artifactory GUI.<br>\n",
    "Next create a .pypirc file inside the root directory in your CLoudera project.<br>\n",
    "Enter below into the .pypirc file:<br>\n",
    "`[distutils]`<br>\n",
    "`index-servers = local`<br>\n",
    "<br>\n",
    "`[local]`<br>\n",
    "`repository:URL_LOCAL_REPO`<br>\n",
    "`username:ENTER_USERNAME`<br>\n",
    "`password:ENTER_PASSWORD`<br>\n",
    "\n",
    "Then run below line in a terminal from your Cloudera project:<br>\n",
    "`python setup.py sdist upload -r local`<br>\n",
    "\n",
    "NOTE: Setup.py should be same as in previous sections.<br>\n",
    "\n",
    "\n",
    "### AWS Batch method\n",
    "\n",
    "This method use AWS cloud architecture including `batch` and `lambda` to schedule a regular job which also employs CI/CD via a docker container connected to `TeamCity` and `Octopus`.\n",
    "\n",
    "#### Docker container of code\n",
    "First step is to setup a working docker on your local VM which works in same way as original code.<br>\n",
    "As a starting point git clone an existing project docker which has been productionised to get template docker setup with base image etc.<br>\n",
    "\n",
    "Create new git repo and use template docker with new project code inside it.<br>\n",
    "\n",
    "__Making template docker project specific__<br>\n",
    "Next replace all mention of the old docker name with your new docker name. This needs to be done in a number of places see below:<br>\n",
    "\n",
    "\n",
    "##### Dockerfile<br>\n",
    "Dockerfile is used to build the docker including installing packages, Kerberos config setup, amazonRedshift setup, environment variables. (Can also add in volumes here as well as in docker-compose.yml file)\n",
    "\n",
    "Add in any additional packages eg Amazon Redshift drivers etc as necessary.<br>\n",
    "Installing packages within Dockerfile<br>\n",
    "Ideally should list packages in requirements.txt file and then the command below in the Dockerfile will install them:<br>\n",
    "`RUN pip3 install -r /FILEPATH/requirements/base.txt`<br>\n",
    "You can also use pip commands within the Dockerfile directly bit this is untidy and adds steps to the build process:<br>\n",
    "`RUN pip3 install selenium`<br>\n",
    "\n",
    "__Note__ if a package cannot be downloaded and installed via pip or apt in this file you can download it outside the docker and install it within this file like below. Here we downloaded twisted-17.1.0 externally and then unzipped it and installed in the Dockerfile meaning this will be done as the docker is built.<br>\n",
    "\n",
    "`ADD ./Twisted-17.1.0 /FILEPATH/Twisted-17.1.0`\n",
    "`ADD ./tar_file.sh /FILEPATH/tar_file.sh`\n",
    "`RUN chmod +x /FILEPATH/tar_file.sh`\n",
    "`RUN /FILEPATH/tar_file.sh`\n",
    "\n",
    "In this example the tar_file.sh is simple a bash script with the below lines to install twisted as part of the docker build process.<br>\n",
    "\n",
    "`cd Twisted-17.1.0`<br>\n",
    "`python3 setup.py install`<br>\n",
    "\n",
    "To run scripts from inside dockerfile automatically each time docker is built:<br>\n",
    "`ENTRYPOINT [\"/FILEPATH/docker-entrypoint.sh\"]` <br>\n",
    "This also works<br>\n",
    "`ENTRYPOINT [\"python\", \"main.py\"]`<br> \n",
    "\n",
    "\n",
    "__Adding volumes inside Dockerfile__<br>\n",
    "Change names of mounted directories as necessary eg<br>\n",
    "`ADD DIR_NAME_IN_DOCKER /FULL_FILE_PATH_ON_SYSTEM`<br>\n",
    "\n",
    "\n",
    "\n",
    "##### Makefile\n",
    "Makefile defines list of any commands that can be passed to the docker container externally for example to use dev-bash listed below just type make dev-bash (see example below). Simply place make in front of any command to run it.<br>\n",
    "\n",
    "The code below basically means when you type make dev-bash in the root docker folder it will run the line of code below dev-bash below.<br>\n",
    "\n",
    "`dev-bash:`<br>\n",
    "`docker-compose run --rm --entrypoint \"/bin/bash -c\" DOCKER_NAME bash`<br>\n",
    "\n",
    "Main command is make run-main which will run main.py inside docker. Note main.py is only visible inside docker as it is mounted in the base image\n",
    "\n",
    "Change name of docker in this file and this must be the same as service name in the docker-compose.yml file.<br>\n",
    "\n",
    "\n",
    "\n",
    "##### Click commands\n",
    "Click commands are decorators which enable commands to be passed via the command line. See example below for how it might run a main.py script in the docker direct from command line outside the docker.<br>\n",
    "\n",
    "`Python main.py batch run_main`<br>\n",
    "\n",
    "The process this line starts is as follows:<br>\n",
    "•\tIt first runs the main.py file in the home directory of the docker where the click commands are stored.<br> \n",
    "•\tIt then goes into app/commands and runs run_main from the code_commands.py script<br>\n",
    "•\tThe run_main function then runs the main function inside the Code folder<br>\n",
    "•\tThis final main function runs the project code.<br>\n",
    "\n",
    "\n",
    "##### App/Commands\n",
    "These use click commands just like makefile, structure of base image is that it looks here and runs the python script in this directory.<br>\n",
    "\n",
    "Change name of python script eg from DOCKER_NAME_commands to NEW_NAME, the main.py script goes into this directory and runs any scripts with .py suffix so name of it not that important.\n",
    "Inside script change imports to be new name eg \n",
    "From DOCKER_NAME.main import main to from NEW_NAME.main import main, this name is same as name in docker root directory where code stored in this case DOCKER_NAME.\n",
    "Change comments as necessary\n",
    "\n",
    "\n",
    "##### Docker\n",
    "Holds some bash scripts that are run from Dockerfile eg unset_proxies.sh<br>\n",
    "Add in extra bash scripts and reference them in Dockerfile if required\n",
    "\n",
    "\n",
    "##### Setup.py\n",
    "Holds details of docker and author. NOt used here as not deploying as package.\n",
    "\n",
    "\n",
    "##### Docker-compose.yml\n",
    "specifies the different services offered, can add in volumes(directories) you want to appear in inside the docker here too as alternative to mounting them in the Dockerfile.<br>\n",
    "\n",
    "Change service name to new project. This same name will be used in the Makefile to launch the docker.<br>\n",
    "Change name references to code_folder to whatever name of directory containing code is called.<br>\n",
    "Add in args and volumes as necessary eg. Below where no_proxy has been added as arg and Stanford has been added as volume<br>\n",
    "\n",
    "`args:`<br>\n",
    "     `http_proxy: ${http_proxy}`<br>\n",
    "     `https_proxy: ${https_proxy}`<br>\n",
    "     `no_proxy: ${no_proxy}`<br>\n",
    "     `BUILD_NUMBER: ${BUILD_NUMBER}`<br>\n",
    "     `BUILD_NUMBER_TAG: ${BUILD_NUMBER_TAG}`<br>\n",
    "     `BUILD_VCS_NUMBER: ${BUILD_VCS_NUMBER}`<br>\n",
    "`volumes:`<br>\n",
    "     `- ./PATH_TO_FOLDER_1_INSIDE_DOCKER:/PATH_TO_FOLDER_1_OUTSIDE_DOCKER`<br>\n",
    "     `- ./PATH_TO_FOLDER_2_INSIDE_DOCKER:PATH_TO_FOLDER_2_OUTSIDE_DOCKER`<br>\n",
    "     `- ./PATH_TO_FOLDER_3_INSIDE_DOCKER:PATH_TO_FOLDER_3_OUTSIDE_DOCKER`<br>\n",
    "\n",
    "##### jobmodel\n",
    "Contains job-definition json needs renamed new project name and also contents need updated if new variables declared such as no_proxy see below\n",
    "       {\n",
    "          \"name\": \"no_proxy\",\n",
    "          \"value\": \"#{NO_PROXY}\"\n",
    "        },\n",
    "REMEMBER ANY NEW VARIABLES NEED DECLARED HERE AND IN TEAM CITY WITH THE TEAM CITY VALUE BEING USED IN THE CODE.\n",
    "\n",
    "#### TeamCity\n",
    "Go to Administration top right of screen then choose correct project template and then create subproject.<br>\n",
    "Link to bitbucket repository pasting in the clone link into repo URL <br>           \n",
    "Fill in project name <br>\n",
    "Enter build config name as Build<br>\n",
    "\n",
    "Do not auto-detect build steps. Instead go to parameters and attach docker Compose template and click associate.<br>       \n",
    "         \n",
    "__Build parameters__   \n",
    "Complete below sections:<br>\n",
    "Build – General settings <br>     \n",
    "Build – Steps (inherited automatically when attach template  <br>           \n",
    "Build – Triggers (inherited automatically when attach template) <br>           \n",
    "Build – Features (inherited automatically when attach template) <br>\n",
    "               \n",
    "__Agent requirements__\n",
    "Then assign relevant agents to your project name\n",
    "               \n",
    "#### Octopus\n",
    "Add new project<br>            \n",
    "Choose to add a step and choose AWS – Deploy to batch<br>\n",
    "Complete project variables giving Dev and Production scope to all variables <br>\n",
    "Complete settings.<br>\n",
    "               \n",
    "#### AWS Lambda\n",
    "Search for generic lambda function which runs code linked in bitbucket.<br>\n",
    "Click on configure test events.<br>               \n",
    "Then give it an event name and copy and paste in the job model json from the docker<br>\n",
    "Note must change request ID for each new test eg below would be updated to build_models_2<br>\n",
    "              \n",
    "#### AWS Batch\n",
    "To monitor progress go to batch on AWS and click on your job in que, then look at the cloudwatch logs.<br>\n",
    "If you are debugging use logging.info inside your code to print out variables etc in the log files to check what code is doing ( see example below for printing out the value of the proxy variables)<br>\n",
    "        `logging.info('PRINTING HTTP PROXY' + str(os.environ['http_proxy']))`<br>\n",
    "        `logging.info('PRINTING HTTPS PROXY' + str(os.environ['https_proxy']))`<br>\n",
    "        `logging.info('PRINTING NO PROXY' + str(os.environ['no_proxy']))`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communications packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ZMQ pub/sub model for message transfer\n",
    "#### Useful links\n",
    "https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/patterns/pubsub.html<br>\n",
    "\n",
    "#### Simple pub/sub example using sockets\n",
    "\n",
    "To setup broadcasting port<br>\n",
    "`context = zmq.Context()`<br>\n",
    "`socket = context.socket(zmq.PUB)`<br>\n",
    "`socket.connect('tcp://127.0.0.1:1111')`<br>\n",
    "\n",
    "To send message from source<br>\n",
    "`data = raw_data.to_json()`<br>\n",
    "`socket.send_string(data)`<br> \n",
    "\n",
    "To start listening to selected port<br>\n",
    "`context = zmq.Context()`<br>\n",
    "`socket = context.socket(zmq.SUB)`<br>\n",
    "`socket.bind('tcp://127.0.0.1:1111')`<br>\n",
    "`socket.setsockopt_string(zmq.SUBSCRIBE, '')`<br>\n",
    "\n",
    "To convert incoming message to a dataframe for manipulstion in python<br>\n",
    " `msg = socket.recv_string()`<br>\n",
    " `msg = json.loads(msg)`<br>\n",
    " `msg = pd.DataFrame.from_dict(msg)`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic email updates in python using smtlib\n",
    "#### Useful Links\n",
    "https://nitratine.net/blog/post/how-to-send-an-email-with-python/#html-in-emails<br>\n",
    "https://www.freecodecamp.org/news/send-emails-using-code-4fcea9df63f/<br>\n",
    "https://linuxhint.com/sending-email-python/<br>\n",
    "\n",
    "#### Import necessaary libraries including smtlib\n",
    "`import smtlib`<br>\n",
    "`from email.mine.multipart import MIMEMultipart`<br>\n",
    "`from email.mine.text import MIMEText`<br>\n",
    "`from email.mine.base import MIMEBase`<br>\n",
    "<br>\n",
    "#### Create the MIMEMultipart message object and load it with appropriate headers for From, To, and Subject fields\n",
    "Multipurpose Internet Mail Extensions (MIME) is an Internet standard that is used to support the transfer of single or multiple text and non-text attachments. Non-text attachments can include graphics, audio, and video files. You can send multiple attachments in a single ebMS message by using the MIME implementation in B2B Advanced Communications.<br>\n",
    "This is essentially the part where you insert any attachments you need as well as the body of the email itself.<br>\n",
    "<br>\n",
    "#### Set up the SMTP server and log into your account\n",
    "To send the email, you need to make use of SMTP (Simple Mail Transfer Protocol). As mentioned earlier, Python provides libraries to handle this task.<br>\n",
    "<br>\n",
    "#### Send the message using the SMTP server object\n",
    "Finally combine the message content with the SMTP Server to send the email.<br>\n",
    "<br>\n",
    "Below is a template function where:<br>\n",
    "1) Some text is attached as html to the body of the email<br>\n",
    "2) A CSV is aadded as an attachment<br>\n",
    "3) A PNG file is added as an attachment<br>\n",
    "<br>\n",
    "Example usage of the function is `email_update(f'file_1.csv', 'pic_1.png',   text/table as email body)`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_update(attachment, attachment_2, content):\n",
    "    #Setup the MIME with the message content\n",
    "    message = MIMEMultipart()\n",
    "        \n",
    "    #To, From and Subject fields\n",
    "    sender_address = 'source_email_address'\n",
    "    sender_pass = 'your_account_password'\n",
    "    receiver_address = 'destination_email_address'\n",
    "    message['From'] = sender_address\n",
    "    message['To'] = receiver_address\n",
    "    message['Subject'] = 'Email_subject'\n",
    "    \n",
    "    #The body and the attachments for the mail\n",
    "\n",
    "    #Part 1: Attach text in body of email which in this case ends up being the tail of a pandas dataframe\n",
    "    mail_content =\"\"\"\\\n",
    "    <html>\n",
    "      <head></head>\n",
    "      <body>\n",
    "        {0}\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\".format(content.to_html())\n",
    "    message.attach(MIMEText(mail_content, 'html'))\n",
    "    \n",
    "    #Part 2 Attach CSV file\n",
    "    attach_file_name = attachment\n",
    "    # Open the file in binary mode\n",
    "    attach_file = open('csv file to attach.csv', 'rb') \n",
    "    payload = MIMEBase('application', 'csv', Name='file name in email(can be different to original file name in previous line).png')\n",
    "    payload.set_payload((attach_file).read())\n",
    "    #encode the attachment\n",
    "    encoders.encode_base64(payload) \n",
    "    #add payload header with filename\n",
    "    payload.add_header('Content-Decomposition', 'attachment', filename=attach_file_name)\n",
    "    message.attach(payload)\n",
    "\n",
    "    #Part 3 Attach png file\n",
    "    attach_file_name_2 = attachment_2\n",
    "    attach_file = open('png file to attach.png', 'rb') # Open the file as binary mode\n",
    "    payload = MIMEBase('application', 'png', Name='file name in email(can be different to original file name in previous line).png')\n",
    "    payload.set_payload((attach_file).read())\n",
    "    #encode the attachment\n",
    "    encoders.encode_base64(payload) \n",
    "    #add payload header with filename\n",
    "    payload.add_header('Content-Decomposition', 'attachment_2', filename=attach_file_name_2)\n",
    "    message.attach(payload)\n",
    "    \n",
    "    #Create SMTP session for sending the mail\n",
    "    #use gmail with port\n",
    "    session = smtplib.SMTP('smtp.gmail.com', 587) \n",
    "    #enable security\n",
    "    session.starttls() \n",
    "    #login with mail_id and password\n",
    "    session.login(sender_address, sender_pass) \n",
    "    text = message.as_string()\n",
    "    session.sendmail(sender_address, receiver_address, text)\n",
    "    session.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Server (Using Dash, Flask, Gunicorn, Nginz)\n",
    "\n",
    "### Useful Links to be checked and moved where necessary\n",
    "\n",
    "https://pythonprogramming.net/deploy-vps-dash-data-visualization/<br>\n",
    "https://community.plot.ly/t/hosting-multiple-dash-apps-with-uwsgi-nginx/6758<br>\n",
    "https://github.com/aio-libs/aiohttp/issues/3291<br>\n",
    "https://github.com/plotly/dash/issues/214<br>\n",
    "https://www.shanelynn.ie/asynchronous-updates-to-a-webpage-with-flask-and-socket-io/<br>\n",
    "https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-gunicorn-and-nginx-on-ubuntu-18-04<br>\n",
    "https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-18-04<br>\n",
    "https://www.digitalocean.com/community/questions/flask-and-http-basic-authentication<br>\n",
    "https://dash.plot.ly/urls<br>\n",
    "https://www.rbtechblog.com/blog/deploy_bokeh_app#section3<br>\n",
    "https://pusher.com/tutorials/bitcoin-live-graph-python<br>\n",
    "https://www.digitalocean.com/community/tutorials/how-to-set-up-zoho-mail-with-a-custom-domain-managed-by-digitalocean-dns<br>\n",
    "https://www.cyberciti.biz/faq/nginx-restart-ubuntu-linux-command/<br>\n",
    "https://sidhenriksen.github.io/datascience/2017/11/14/hosting-multiple-dash-apps-in-the-cloud.html<br>\n",
    "https://towardsdatascience.com/how-to-build-a-complex-reporting-dashboard-using-dash-and-plotl-4f4257c18a7f<br>\n",
    "https://medium.com/@aliciagilbert.itsimplified/build-stunning-interactive-web-data-dashboards-with-python-plotly-and-dash-fcbdc09ba318<br>\n",
    "https://plot.ly/python/creating-and-updating-figures/<br>\n",
    "https://stackoverflow.com/questions/6700149/python-zeromq-multiple-publishers-to-a-single-subscriber<br>\n",
    "https://www.shanelynn.ie/asynchronous-updates-to-a-webpage-with-flask-and-socket-io/<br>\n",
    "https://community.plot.ly/t/valueerror-received-for-the-x-property-of-scatter-zeromq-live-chart/11692/3<br>\n",
    "https://docs.google.com/document/d/1DjWL2DxLiRaBrlD3ELyQlCBRu7UQuuWfgjv9LncNp_M/edit<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Web Server (Using Dash, Flask, Gunicorn, Nginz)\n",
    "\n",
    "### Useful Links\n",
    "https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-gunicorn-and-nginx-on-ubuntu-18-04\n",
    "\n",
    "### Overview\n",
    "A python web application consists of the following three main components listed below (__Dash/Flask app, WSGI application server, web server__). <br>\n",
    "It is important to note that while lightweight and easy to use, Flask’s built-in server is not suitable for production as it doesn’t scale well and by default serves only one request at a time, hence the need for a application server and web server as shown below. This is the reason why when you launch a Flask/Dash app without the parts below you get the following warning<br>\n",
    "\n",
    "`Serving Flask app \"hello_flask\" (lazy loading)`<br>\n",
    "`* Environment: production`<br>\n",
    "`  WARNING: Do not use the development server in a production environment.`<br>\n",
    "`  Use a production WSGI server instead.`<br>\n",
    "\n",
    "#### python application (like Dash/Flask)\n",
    "This will be a python script such as the one listed later as an example in this section. This file contains the code to read in the data and to display it as a dashboard etc. It will be responsible for login authorisation as well as links between graphs and website navigation etc.\n",
    "<br>\n",
    "\n",
    "\n",
    "#### A WSGI application server (like Gunicorn)\n",
    "Gunicorn translates requests which it gets from Nginx into a format which your web application can handle, and makes sure that your code is executed when needed.<br>\n",
    "Once Nginx decides, that a particular request should be passed on to Gunicorn (due to the rules you configured it with), it’s Gunicorn’s time to shine.<br>\n",
    "Gunicorn is really great at what it does! It’s highly optimized and has a lot of convenient features. Mostly, its jobs consist of:<br>\n",
    "Running a pool of worker processes/threads (executing your code!)<br>\n",
    "Translates requests coming in from Nginx to be WSGI compatible<br>\n",
    "Translate the WSGI responses of your app into proper http responses<br>\n",
    "Actually calls your Python code when a request comes in<br>\n",
    "Gunicorn can talk to many different web servers<br>\n",
    "What Gunicorn can’t do for you:<br>\n",
    "Not prepared to be front-facing: easy to DOS and overwhelm<br>\n",
    "Can’t terminate SSL (no https handling)<br>\n",
    "Do the job of a webserver like Nginx, they are better at it<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### A web server (like nginx)\n",
    "Nginx is where requests from the internet arrive first. It can handle them very quickly, and is usually configured to only let those requests through, which really need to arrive at your web application.<br>\n",
    "Nginx is a web server and reverse proxy. It’s highly optimized for all the things a web server needs to do. Here are a few things it’s great at:<br>\n",
    "Take care of domain name routing (decides where requests should go, or if an error response is in order)<br>\n",
    "Serve static files<br>\n",
    "Handle lots of requests coming in at once<br>\n",
    "Handle slow clients<br>\n",
    "Forwards requests which need to be dynamic to Gunicorn<br>\n",
    "Terminate SSL (https happens here)<br>\n",
    "Save computing resources (CPU and memory) compared to your Python code<br>\n",
    "And a lot more, if you configure it to do so (load balancing, caching, …)<br>\n",
    "Things Nginx can’t do for you:<br>\n",
    "Running Python web applications for you<br>\n",
    "Translate requests to WSGI<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Step by step guide to building a production ready web server\n",
    "####  Prerequisites\n",
    "Before starting this guide, you should have the following<br>\n",
    "A server with Ubuntu 18.04 or higher installed and a non-root user with sudo privileges. I have previously used Digital Ocean and their setup guide is very good.<br>\n",
    "Nginx installed, following Steps 1 and 2 of How To Install Nginx on Ubuntu 18.04.<br>\n",
    "A domain name configured to point to your server. You can purchase one on Namecheap or get one for free on Freenom. You can learn how to point domains to DigitalOcean by following the relevant documentation on domains and DNS. Be sure to create the following DNS records:<br>\n",
    "<br>\n",
    "An A record with your_domain pointing to your server’s public IP address.<br>\n",
    "An A record with www.your_domain pointing to your server’s public IP address.<br>\n",
    "Familiarity with the WSGI specification, which the Gunicorn server will use to communicate with your Flask application. This discussion covers WSGI in more detail.<br>\n",
    "\n",
    "#### Installing the Components from the Ubuntu Repositories\n",
    "`sudo apt update`<br>\n",
    "`sudo apt install python3-pip python3-dev build-essential libssl-dev libffi-dev python3-setuptools`<br>\n",
    "\n",
    "#### Creating a Python Virtual Environment\n",
    "The next few steps until instructed otherwise should be completed in the virtualenv we are about to create.(See section on virtual environments)<br>\n",
    "`mkvirtualenv -p /usr/bin/python3.6 web_env`<br>\n",
    "\n",
    "#### Setting Up a Flask Application\n",
    "First, let’s install wheel with the local instance of pip to ensure that our packages will install even if they are missing wheel archives:<br>\n",
    "`pip install wheel`<br>\n",
    "`pip install gunicorn flask`<br>\n",
    "\n",
    "#### Creating a Sample App\n",
    "Now that you have Flask available, you can create a simple application. Flask is a microframework. It does not include many of the tools that more full-featured frameworks might, and exists mainly as a module that you can import into your projects to assist you in initializing a web application.<br>\n",
    "\n",
    "Please see below for example script which would sit inside gemini_app.py script. The script below takes live streaming trade data <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dash'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-28712532bd3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Import required packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdash_core_components\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdcc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdash_html_components\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependencies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dash'"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_auth\n",
    "from flask import Flask\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "try:\n",
    "    data = pd.HDFStore('file_path/data.h5','r')\n",
    "    df = algo_data['data']\n",
    "    data.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "un_pw = [['user_1', 'password_1'], \n",
    "         ['user_2', 'password_2'], \n",
    "         ['user_3', 'password_3']]\n",
    "\n",
    "# USing Dash app as allows more functionality\n",
    "app = dash.Dash(__name__)\n",
    "auth = dash_auth.BasicAuth(app, un_pw)\n",
    "\n",
    "intro = '''Text description of site will appear on landing page'''\n",
    "app.layout = html.Div(children=[\n",
    "html.H1(children='Page Title'),\n",
    "html.H1(id='live-update-text'),\n",
    "dcc.Markdown(children=intro),\n",
    "dcc.Graph(id='graph_set_1',style={'width':800}),\n",
    "dcc.Interval(id='interval-component',\n",
    "            interval=3600000, # 2000 milliseconds = 2 seconds\n",
    "            n_intervals=0)\n",
    "])\n",
    "\n",
    "@app.callback(Output('graph_set_1', 'figure'),\n",
    "                      [Input('interval-component', 'n_intervals')])\n",
    "def update_layout(n):\n",
    "    algo_data = pd.HDFStore('/root/algo_library/algo_version_1/results/summary_trade_data.h5','r')\n",
    "    algo_df = algo_data['summary_trade_data']\n",
    "    algo_data.close()\n",
    "    figure={\n",
    "     'data': [\n",
    "             {'x':df.index, 'y':df['strat_ret'], 'type': 'line', 'name': 'Data series label'},\n",
    "             {'x':df.index, 'y':df['gross_ret'], 'type': 'line', 'name': 'Data series label'},\n",
    "             {'x':df.index, 'y':df['coin_ret'],  'type': 'line', 'name': 'Data series label'}\n",
    "             ],\n",
    "             'layout': {\n",
    "             'title': 'Graph Title'\n",
    "             }\n",
    "             }\n",
    "    return figure\n",
    "\n",
    "@app.callback(Output('live-update-text', 'children'),\n",
    "                      [Input('interval-component', 'n_intervals')])\n",
    "def update_layout_2(n):\n",
    "    now = datetime.datetime.now()\n",
    "    now = now.ctime()\n",
    "    return f'last update time: {now}'\n",
    "\n",
    "server = app.server\n",
    "if __name__ == '__main__':\n",
    "# 80 is the default port so this will be what appears if the url is typed with no port\n",
    "    app.run_server(debug=True, host='0.0.0.0', port='80')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup WSGI Endpoint\n",
    "The above code would only launch a dev server. To properly run a website you should use a production server which can scal as previously discussed. In order to do this we nneed to now create our WSGI application server (Gunicorn) and then our web server (Nginx).<br>\n",
    "\n",
    "We now need to create an entrypoint for our application, this will tell our Gunicorn server how to interact with the application.\n",
    "In same folder as you created your flash/dash app file create a wsgi.py and insert the following code.<br>\n",
    "\n",
    "`from flask_app import server as app`<br>\n",
    "<br>\n",
    "`if __name__ == \"__main__\":`<br>\n",
    " `       app.run()`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring Gunicorn\n",
    "https://vsupalov.com/what-is-gunicorn/<br>\n",
    "https://www.fullstackpython.com/green-unicorn-gunicorn.html<br>\n",
    "Your application is now written with an entry point established. We can now move on to configuring Gunicorn.<br>\n",
    "Before moving on, we should check that Gunicorn can serve the application correctly.<br>\n",
    "We can do this by simply passing it the name of our entry point. This is constructed as the name of the module (minus the .py extension), plus the name of the callable within the application. In our case, this is wsgi:app.<br>\n",
    "We’ll also specify the interface and port to bind to so that the application will be started on a publicly available interface:<br>\n",
    "`cd /dash_flask_app`<br>\n",
    "`gunicorn --bind 0.0.0.0:80 wsgi:app`<br>\n",
    "\n",
    "If you now visit your servers IP address you should see your app running.__You can now leave the virtualenv__<br>\n",
    "\n",
    "Next create the systemd service unit file which will mean ubuntu will automatically start Gunicorn and the Flask app on boot. Call it gemini_app.service<br>\n",
    "\n",
    "`sudo nano /etc/systemd/system/myproject.service`<br>\n",
    "\n",
    "The sections below perform the following actions<br>\n",
    "__Unit section__ is used to specify metadata and dependencies. Let’s put a description of our service here and tell the init system to only start this after the networking target has been reached.<br>\n",
    "\n",
    "__The Service section__. This will specify the user and group that we want the process to run under. Let’s give our regular user account ownership of the process since it owns all of the relevant files. Let’s also give group ownership to the www-data group so that Nginx can communicate easily with the Gunicorn processes. Remember to replace the username here with your username.<br>\n",
    "Next, let’s map out the working directory and set the PATH environmental variable so that the init system knows that the executables for the process are located within our virtual environment. Let’s also specify the command to start the service. This command will do the following.<br>\n",
    "Start 3 worker processes (though you should adjust this as necessary)<br>\n",
    "Create and bind to a Unix socket file, myproject.sock, within our project directory. We’ll set an umask value of 007 so that the socket file is created giving access to the owner and group, while restricting other access<br>\n",
    "Specify the WSGI entry point file name, along with the Python callable within that file (wsgi:app)<br>\n",
    "Systemd requires that we give the full path to the Gunicorn executable, which is installed within our virtual environment.\n",
    "Remember to replace the username and project paths with your own information<br>\n",
    "\n",
    "__Install section__ will tell systemd what to link this service to if we enable it to start at boot. We want this service to start when the regular multi-user system is up and running.<br>\n",
    "\n",
    "\n",
    "`[Unit]`<br>\n",
    "`Description=Gunicorn instance to serve myproject`<br>\n",
    "`After=network.target`<br>\n",
    "<br>\n",
    "`[Service]`<br>\n",
    "`User=sammy`<br>\n",
    "`Group=www-data`<br>\n",
    "`WorkingDirectory=/root/apps/my_project`<br>\n",
    "`Environment=\"PATH=/root/.virtualenvs/web_env/bin\"`<br>\n",
    "`ExecStart=/root/.virtualenvs/web_env/bin/gunicorn --workers 3 --bind unix:myproject.sock -m 007 wsgi:app`<br>\n",
    "<br>\n",
    "`[Install]`<br>\n",
    "`WantedBy=multi-user.target`<br>\n",
    "\n",
    "We can now start the Gunicorn service we created and enable it so that it starts at boot<br>\n",
    "\n",
    "`sudo systemctl start my_project`<br>\n",
    "`sudo systemctl enable my_project`<br>\n",
    "\n",
    "Let’s check the status<br>\n",
    "\n",
    "`sudo systemctl status myproject`<br>\n",
    "You should see output like this<br>\n",
    "\n",
    "`myproject.service - Gunicorn instance to serve myproject`<br>\n",
    " `  Loaded: loaded (/etc/systemd/system/myproject.service; enabled; vendor preset: enabled)`<br>\n",
    "  ` Active: active (running) since Fri 2018-07-13 14:28:39 UTC; 46s ago`<br>\n",
    "` Main PID: 28232 (gunicorn)`<br>\n",
    " `   Tasks: 4 (limit: 1153)`<br>\n",
    "  ` CGroup: /system.slice/myproject.service`<br>\n",
    "   `        ├─28232 /home/sammy/myproject/myprojectenv/bin/python3.6 /home/sammy/myproject/myprojectenv/bin/gunicorn --workers 3 --bind `unix:myproject.sock -m 007`<br>\n",
    " `          ├─28250 /home/sammy/myproject/myprojectenv/bin/python3.6 /home/sammy/myproject/myprojectenv/bin/gunicorn --workers 3 --bind `unix:myproject.sock -m 007`<br>\n",
    " `          ├─28251 /home/sammy/myproject/myprojectenv/bin/python3.6 /home/sammy/myproject/myprojectenv/bin/gunicorn --workers 3 --bind `unix:myproject.sock -m 007`<br>\n",
    " `          └─28252 /home/sammy/myproject/myprojectenv/bin/python3.6 /home/sammy/myproject/myprojectenv/bin/gunicorn --workers 3 --bind `unix:myproject.sock -m 007`<br>\n",
    "\n",
    "If you see any errors, be sure to resolve them before continuing with the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Nginx to proxy requests\n",
    "\n",
    "Our Gunicorn application server should now be up and running, waiting for requests on the socket file in the project directory. Let’s now configure Nginx to pass web requests to that socket by making some small additions to its configuration file.\n",
    "Begin by creating a new server block configuration file in Nginx’s sites-available directory.<br>\n",
    "\n",
    "`vim /etc/nginx/sites-available/myproject`<br>\n",
    "\n",
    "Open up a server block and tell Nginx to listen on the default port 80. Let’s also tell it to use this block for requests for our server’s domain name<br>\n",
    "\n",
    "`/etc/nginx/sites-available/myproject`<br>\n",
    "`server {`<br>\n",
    "`    listen 80;`<br>\n",
    "`    server_name your_domain www.your_domain;`<br>\n",
    "`}`<br>\n",
    "Next, let’s add a location block that matches every request. Within this block, we’ll include the proxy_params file that specifies some general proxying parameters that need to be set. We’ll then pass the requests to the socket we defined using the proxy_pass directive<br>\n",
    "\n",
    "`/etc/nginx/sites-available/myproject`<br>\n",
    "`server {`<br>\n",
    "`    listen 80;`<br>\n",
    "`    server_name your_domain www.your_domain;`<br>\n",
    "\n",
    " `   location / {`<br>\n",
    " `       include proxy_params;`<br>\n",
    " `       proxy_pass http://unix:/home/sammy/myproject/myproject.sock;`<br>\n",
    " `   }`<br>\n",
    "`}`<br>\n",
    "Save and close the file when you’re finished.<br>\n",
    "\n",
    "To enable the Nginx server block configuration you’ve just created, link the file to the sites-enabled directory<br>\n",
    "\n",
    "`sudo ln -s /etc/nginx/sites-available/myproject /etc/nginx/sites-enabled`<br>\n",
    "With the file in that directory, you can test for syntax errors<br>\n",
    "\n",
    "`sudo nginx -t`<br>\n",
    "If this returns without indicating any issues, restart the Nginx process to read the new configuration:<br>\n",
    "\n",
    "`sudo systemctl restart nginx`<br>\n",
    "Finally, let’s adjust the firewall again. We no longer need access through port 5000, so we can remove that rule. We can then allow full access to the Nginx server<br>\n",
    "\n",
    "`sudo ufw delete allow 5000`<br>\n",
    "`sudo ufw allow 'Nginx Full'`<br>\n",
    "You should now be able to navigate to your server’s domain name in your web browser<br>\n",
    "\n",
    "http://your_domain <br>\n",
    "You should see your application’s output<br>\n",
    "If you encounter any errors, trying checking the following<br>\n",
    "\n",
    "`sudo less /var/log/nginx/error.log: checks the Nginx error logs`.<br>\n",
    "`sudo less /var/log/nginx/access.log: checks the Nginx access logs.`<br>\n",
    "`sudo journalctl -u nginx: checks the Nginx process logs.`<br>\n",
    "`sudo journalctl -u myproject: checks your Flask app’s Gunicorn logs.`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Securing the Application with SSL encryption from Lets Encrypt\n",
    "To ensure that traffic to your server remains secure, let’s get an SSL certificate for your domain. There are multiple ways to do this, including getting a free certificate from Let’s Encrypt, generating a self-signed certificate, or buying one from another provider and configuring Nginx to use it by following Steps 2 through 6 of  How to Create a Self-signed SSL Certificate for Nginx in Ubuntu 18.04. We will go with option one for the sake of expediency.<br>\n",
    "\n",
    "First, add the Certbot Ubuntu repository<br>\n",
    "\n",
    "`sudo add-apt-repository ppa:certbot/certbot`<br>\n",
    "You’ll need to press ENTER to accept.<br>\n",
    "\n",
    "Install Certbot’s Nginx package with apt<br>\n",
    "\n",
    "`sudo apt install python-certbot-nginx`<br>\n",
    "Certbot provides a variety of ways to obtain SSL certificates through plugins. The Nginx plugin will take care of reconfiguring Nginx and reloading the config whenever necessary. To use this plugin, type the following<br>\n",
    "\n",
    "`sudo certbot --nginx -d your_domain -d www.your_domain`<br>\n",
    "This runs certbot with the --nginx plugin, using -d to specify the names we’d like the certificate to be valid for.\n",
    "\n",
    "If this is your first time running certbot, you will be prompted to enter an email address and agree to the terms of service. After doing so, certbot will communicate with the Let’s Encrypt server, then run a challenge to verify that you control the domain you’re requesting a certificate for.<br>\n",
    "\n",
    "If that’s successful, certbot will ask how you’d like to configure your HTTPS settings<br>\n",
    "\n",
    "\n",
    "`Please choose whether or not to redirect HTTP traffic to HTTPS, removing HTTP access.\n",
    "\n",
    "`1: No redirect - Make no further changes to the webserver configuration.`<br>\n",
    "`2: Redirect - Make all requests redirect to secure HTTPS access. Choose this for new sites, or if you're confident your site works on HTTPS. You can undo this change by editing your web server's configuration.`<br>\n",
    "Select the appropriate number [1-2] then [enter] (press 'c' to cancel):`<br>\n",
    "\n",
    "Select your choice then hit ENTER. The configuration will be updated, and Nginx will reload to pick up the new settings. certbot will wrap up with a message telling you the process was successful and where your certificates are stored<br>\n",
    "\n",
    "`IMPORTANT NOTES:`<br>\n",
    "` - Congratulations! Your certificate and chain have been saved at:`<br>\n",
    "`   /etc/letsencrypt/live/your_domain/fullchain.pem`<br>\n",
    "`   Your key file has been saved at:`<br>\n",
    "`   /etc/letsencrypt/live/your_domain/privkey.pem`<br>\n",
    "`   Your cert will expire on 2018-07-23. To obtain a new or tweaked`<br>\n",
    "`   version of this certificate in the future, simply run certbot again`<br>\n",
    "`   with the \"certonly\" option. To non-interactively renew *all* of`<br>\n",
    " `  your certificates, run \"certbot renew\"`<br>\n",
    " `- Your account credentials have been saved in your Certbot`<br>\n",
    " `  configuration directory at /etc/letsencrypt. You should make a`<br>\n",
    " `  secure backup of this folder now. This configuration directory will`<br>\n",
    " `  also contain certificates and private keys obtained by Certbot so`<br>\n",
    " `  making regular backups of this folder is ideal.`<br>\n",
    " `- If you like Certbot, please consider supporting our work by`<br>\n",
    "\n",
    " `  Donating to ISRG / Let's Encrypt:   https://letsencrypt.org/donate`<br>\n",
    " `  Donating to EFF:                    https://eff.org/donate-le`<br>\n",
    "\n",
    "If you followed the Nginx installation instructions in the prerequisites, you will no longer need the redundant HTTP profile allowance:\n",
    "\n",
    "`sudo ufw delete allow 'Nginx HTTP'`<br>\n",
    "To verify the configuration, navigate once again to your domain, using https://:<br>\n",
    "\n",
    "`https://your_domain`<br>\n",
    "You should see your application output once again, along with your browser’s security indicator, which should indicate that the site is secured.<br>\n",
    "\n",
    "From now on to turn the web server off.<br>\n",
    "`sudo systemctl stop nginx`<br>\n",
    "`sudo systemctl stop my_project`<br>\n",
    "\n",
    "From now on to turn the web server on.<br>\n",
    "`sudo systemctl start nginx`<br>\n",
    "`sudo systemctl enable nginx`<br>\n",
    "`sudo systemctl start my_project`<br>\n",
    "`sudo systemctl enable my_project`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log out button link\n",
    "`https://dash.plot.ly/dash-core-components/logoutbutton`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable data science\n",
    "This is a course I completed when Raazesh Sainudiin gave us a face to face course in scalable data science tools including spark, scala and zeppelin notebooks.I have included the most important links for the course below.<br>\n",
    "\n",
    "https://lamastex.github.io/scalable-data-science/sds/2/x/\n",
    "https://community.cloud.databricks.com/login.html#notebook/3206445649578041/command/3206445649578046\n",
    "https://developer.twitter.com/en/account/get-started\n",
    "GitHub - lamastex/mrs2: a C++ class library for statistical set processing and computer-aided proofs in statistics.\n",
    "https://github.com/lamastex/scalable-data-science/blob/master/_sds/basics/infrastructure/onpremise/dockerCompose.zip\n",
    "https://github.com/lamastex/scalable-data-science\n",
    "https://github.com/lamastex/scalable-data-science/tree/master/dbcArchives/2017/parts/studentProjects\n",
    "\n",
    "\n",
    "## Useful links\n",
    "Flint (time series library from two sigma using spark)<br>\n",
    "https://databricks.com/blog/2018/09/11/introducing-flint-a-time-series-library-for-apache-spark.html<br>\n",
    "https://medium.com/@dvainrub/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3<br>\n",
    "https://sundog-education.com/spark-scala/<br>\n",
    "https://www.edx.org/learn/apache-spark<br>\n",
    "zeppelin jetbrains plugin<br>\n",
    "https://plugins.jetbrains.com/plugin/10023-intellij-zeppelin<br>\n",
    "\n",
    "## Important note on compatible verisons\n",
    "It is critical that the versions of scala, spark, hadoop and sbt are compatible. It is not necessarily the case that the most recent versions of each will work together.<br>\n",
    "The latest compatible set of versions are:<br>\n",
    "\n",
    "`spark=2.4.4`<br>\n",
    "`scala=2.13.1`<br>\n",
    "`hadoop=2.7`<br>\n",
    "`sbt=1.3.5`<br>\n",
    "\n",
    "\n",
    "## Initial setup linux\n",
    "https://sundog-education.com/spark-scala/\n",
    "\n",
    "### Java \n",
    "If type which java this points to `/usr/bin/java` however this is symbolic link to `/etc/alternatives/java` which in turn points to `/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java`. This structure is necessary for the way Java operates but be mindful when trying to force a certain version to be the default.\n",
    "\n",
    "Therefore if already have wrong version eg version11 then to uninstall \n",
    "(Notes on uninstall https://novicestuffs.wordpress.com/2017/04/25/how-to-uninstall-java-from-linux/)<br>\n",
    "`sudo apt-get remove openjdk*`<br>\n",
    "`sudo apt-get install openjdk-8-jdk`<br>\n",
    "`sudo apt-get install openjdk-8-jre`<br>\n",
    "(use headless if don’t need gui)<br>\n",
    "\n",
    " ### Spark\n",
    "Download file from: https://spark.apache.org/downloads.html <br>\n",
    "Then extract <br>\n",
    "`tar xvf spark-1.3.1-bin-hadoop2.6.tgz`<br>\n",
    "Then move this file to  `/usr/local/spark`<br>\n",
    "`mv spark-1.3.1-bin-hadoop2.6 /usr/local/spark`<br>\n",
    "Finally add below line to .bashrc<br>\n",
    "`export PATH=$PATH:/usr/local/spark/bin`<br>\n",
    "\n",
    " \n",
    "### Hadoop\n",
    "Download the binary file from link below (Note use generic hadoop folder name without version number as when updating can simply put new version into same hadoop folder without having to update the PATH variable etc.<br>\n",
    "`http://hadoop.apache.org/releases.html`<br>\n",
    "Extract and move to desired location as below\n",
    "`tar xzvf hadoop-3.2.1.tar.gz`<br>\n",
    "`mv hadoop-3.2.1/* ~/FILE_PATH_EXAMPLE/spark_scala/software/hadoop/`<br>\n",
    "\n",
    "Add the Hadoop and Java paths in the bash file (.bashrc).<br>\n",
    "Open. bashrc file. Now, add Hadoop and Java Path as shown below.<br>\n",
    "<br>\n",
    "\n",
    "`export HADOOP_HOME=~/FILE_PATH/training/spark_scala/software/hadoop`<br>\n",
    "`export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:~/FILE_PATH/training/spark_scala/software/hadoop/share/hadoop/common/lib`<br>\n",
    "`export HADOOP_CONF_DIR=~/FILE_PATH/training/spark_scala/software/hadoop/etc/hadoop`<br>\n",
    "`export HADOOP_MAPRED_HOME=~/FILE_PATH/training/spark_scala/software/hadoop`<br>\n",
    "`export HADOOP_COMMON_HOME=~/FILE_PATH/training/spark_scala/software/hadoop`<br>\n",
    "`export HADOOP_HDFS_HOME=~/FILE_PATH/training/spark_scala/software/hadoop`<br>\n",
    "`export YARN_HOME=~/FILE_PATH/training/spark_scala/software/hadoop`<br>\n",
    "`export PATH=$PATH:~/FILE_PATH/training/spark_scala/software/hadoop/bin`<br>\n",
    "\n",
    "### Scala\n",
    "https://www.scala-lang.org/download/\n",
    "\n",
    "__Don’t install using apt as won’t see commands in terminal when using scala__ <br>\n",
    "\n",
    "To fix the problem in the current scala repl session run:<br>\n",
    "`import sys.process._`<br>\n",
    "`\"reset\" !`<br>\n",
    "To fix the problem completely removed scala and install it with dpkg (not with apt):<br>\n",
    "`sudo apt-get remove scala-library scala`<br>\n",
    "`sudo wget www.scala-lang.org/files/archive/scala-2.13.1.deb`<br>\n",
    "`sudo dpkg -i scala-2.13.1.deb`<br>\n",
    "\n",
    "Alternatively to install directly from the binary download from scala link above and extract.<br>\n",
    "(Important note `/usr/bin/scala` is symbolic link to `/usr/share/scala/bin/scala`.\n",
    "Therefore if using below method be sure to move the extracted file to `/usr/share/HERE`)\n",
    "`tar xvf scala-2.11.6.tgz`<br>\n",
    "Next move contents to `/usr/share/scala`<br>\n",
    "\n",
    "Finall add below line to .bashrc:<br>\n",
    "`export SCALA_HOME=/usr/bin/scala`<br>\n",
    " \n",
    " \n",
    "### SBT\n",
    "https://www.scala-sbt.org/download.html<br>\n",
    "https://www.scala-sbt.org/1.0/docs/Installing-sbt-on-Linux.html <br>\n",
    "\n",
    "`echo \"deb https://d1.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list`<br>\n",
    "`sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823`<br>\n",
    "`sudo apt-get update`<br>\n",
    "`sudo apt-get install sbt`<br>\n",
    "\n",
    "If having difficutly getting sbt from the repo them simply download it manually from the site https://www.scala-sbt.org/download.html and extract it. As with scala note `/usr/bin/sbt` is a symbolic link to `/usr/share/sbt/bin/sbt`, therefore move the extracted file to `usr/share/HERE`\n",
    "`tar xvf sbt-1.3.5.tgz`<br>\n",
    "Next move contents to `/usr/share/sbt`<br>\n",
    "\n",
    "Finally add following line to .bashrc<br>\n",
    "`Export PATH=$PATH:/usr/local/sbt/bin`<br>\n",
    "\n",
    "\n",
    "### Check installations\n",
    "Update bashrc<br>\n",
    "`Command source ~/.bashrc`<br>\n",
    "\n",
    "check if installed<br>\n",
    "`Command: java -version`<br>\n",
    "`Result: openjdk version \"1.8.0_212\"`<br>\n",
    "`Command: hadoop version`<br>\n",
    "`Result: Hadoop 2.7.3`<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Initial setup Windows\n",
    "https://sundog-education.com/spark-scala/\n",
    "\n",
    "\n",
    "\n",
    "### Java\n",
    "Install the JDK for java 8, do not install Java 9,10 or 11, go-to the following page and download the .exe for windows and follow the installer instructions.<br>\n",
    "Note change the install path to avoid spaces like in `program files` so change to `C:jdk/`<br>\n",
    "https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\n",
    "   \n",
    "<img src=\"media/spark_1.png\">    \n",
    "    \n",
    "After the JDK is installed verify the installation by issuing the following command:<br>\n",
    "`Java -version`<br> \n",
    "\n",
    "### Spark\n",
    "Try and run the next steps in the order they are presented here.<br>\n",
    "Download a pre-built version of Spark from link below<br>\n",
    "https://spark.apache.org/downloads.html\n",
    "    \n",
    "<img src=\"media/spark_2.png\">      \n",
    "\n",
    "Download 7-zip: https://www.7-zip.org/ (note: you may have this installed already)<br>\n",
    "Right click on the `spark-XXXX.tgz` file, Choose `7-zip > extract here`. This should create a `.tar` file in the downloads folder. <br>\n",
    "Right click on the `spark-XXXX.tar` file, Choose `7-zip > extract files`. This should create a folder called `spark-XXXX` in the downloads folder <br>\n",
    "Create a new folder called `spark` in the `C:\\` root folder<br>\n",
    "Copy the contents of `spark-XXXX` folder from the Downloads folder to the `C:\\spark` folder<br>     \n",
    "Rename file `log4j.properties.template` to `log4j.properties` in `C:\\spark\\spark\\conf`<br>\n",
    "Change `log4j.rootCategory=Error, console`<br>\n",
    "\n",
    "<img src=\"media/spark_4.png\">\n",
    "\n",
    "\n",
    "### winutils (Hadoop Support)\n",
    "Download winutils.exe (64bit) to support Hadoop from the following location:<br> \n",
    "https://sundog-spark.s3.amazonaws.com/winutils.exe<br>\n",
    "Copy the winutils.exe file into the c:\\winutils\\bin folder<br>\n",
    "open windows cmd prompt and follwo below steps to trick windows into thinking hadoop is installed and executable<br>\n",
    "`cd c:\\winutils\\bin`<br>\n",
    "`mkdir c:\\tmp\\hive`<br>\n",
    "`winutils.exe chmod 777 c:\\tmp/hive`<br>\n",
    "\n",
    "### Scala\n",
    "Download Scala binary msi from the following link https://www.scala-lang.org/download/ and follow installation instructions.\n",
    "\n",
    "<img src=\"media/spark_7.png\">\n",
    "\n",
    "### SBT \n",
    "Download and install SBT msi from the following link  https://www.scala-sbt.org/download.html\n",
    "\n",
    "<img src=\"media/spark_6.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "### Define all the necessary path variables\n",
    "\n",
    "Right-click your Windows menu, select Control Panel, System and Security, and then System. Click on “Advanced System Settings” and then the “Environment Variables” button.<br>\n",
    "\n",
    "| Variable Name\t| Value\n",
    "|---|---|\n",
    "|SBT_HOME|\tC:\\Program Files (x86)\\sbt|\n",
    "|SCALA_HOME\t|C:\\Program Files (x86)\\scala|\n",
    "|SPARK_HOME|\tC:\\spark\\|\n",
    "|_JAVA_OPTIONS|\t-Xmx512M -Xms512M -Dhttps.proxyPort=8080|\n",
    "|HADOOP_HOME|\tC:\\winutils\\ (set this to same path to where winutils.exe is saved, without the \\bin path)|\n",
    "|JAVA_HOME\t|C:\\jdk|\n",
    "|Add these PATH env variable, colon separated| %SBT_HOME%\\bin %SPARK_HOME%\\bin %SCALA_HOME%\\bin %JAVA_HOME%\\bin|\n",
    "\n",
    "#### System and user variables explained\n",
    "Very similar to how the Registry works on Windows, we have System and User Environment Variables. The system variables are system-wide accepted and do not vary from user to user. Whereas, User Environments are configured differently from user to user. You can add your variables under the user so that other users are not affected by them.<br>\n",
    "\n",
    "Just for your information since we are discussing the topic in depth. System Variables are evaluated before User Variables. So if there are some user variables with the same name as system variables then user variables will be considered. The Path variable is generated in a different way. The effective Path will be the User Path variable appended to the System Path variable. So the order of entries will be system entries followed by user entries.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Check installation of Spark\n",
    "\n",
    "1.\tcd to the directory apache-spark was installed to and then ls to get a directory listing.<br>\n",
    "2.\tLook for a text file we can play with, like README.md or CHANGES.txt<br>\n",
    "3.\tEnter spark-shell<br>\n",
    "4.\tAt this point you should have a scala> prompt. If not, double check the steps above.<br>\n",
    "5.\tEnter val rdd = sc.textFile(“README.md”) (or whatever text file you’ve found) Enter rdd.count()<br>\n",
    "6.\tYou should get a count of the number of lines in that file! <br>\n",
    "7.\tHit control-D to exit the spark shell, and close the console window <br>\n",
    "\n",
    "\n",
    "\n",
    "## IDE Setup \n",
    "\n",
    "__NOTE__\n",
    "Below notes assume spark, scala, sbt and Java already installed.\n",
    "\n",
    "### intellij IDEA\n",
    "\n",
    "#### Windows install\n",
    "Download and follow install instructions.\n",
    "https://www.jetbrains.com/idea/\n",
    "\n",
    "#### Linux install\n",
    "Useful link<br>\n",
    "https://www.javahelps.com/2015/04/install-intellij-idea-on-ubuntu.html<br>\n",
    "Download binary file from below link<br>\n",
    "https://www.jetbrains.com/idea/download/download-thanks.html?platform=linux&code=IIC<br>\n",
    "`cd /opt`<br>\n",
    "`sudo tar -xvzf ~/Downloads/ideaIC-2018.3.2.tar.gz`<br>\n",
    "`sudo mv idea-IC-183.4886.37 idea`<br>\n",
    "`/opt/idea/bin/idea.sh`<br>\n",
    "In the appeared dialog to import existing settings, choose \"Do not import settings\" if you want a fresh installation. If you already had an IntelliJ IDEA, you can import the previous settings by selecting the first option.<br>\n",
    "In the next dialog, you will be asked to select a UI theme. Depending on your preference, select the theme and click Next.<br>\n",
    "Now, you will be provided an option to create a Desktop Entry. I prefer to create the Desktop Entry for all users. Therefore, I select the \"For all users...\" option and click Next.<br>\n",
    "If you want to launch IntelliJ IDEA from Terminal, creating a launcher script makes your life easier. Depending on your requirement, you can enable this feature.<br>\n",
    "Now you can customize existing plugins. However, I do not find any reason to customize them at this point. So just click Next.<br>\n",
    "At this step, you can install additional plugins. If you are using IntelliJ only for Java development, you can skip this step and click Start Using IntelliJ IDEA.Once you have clicked the Start Using IntelliJ IDEA button, you may be asked to enter the root password in order to create desktop entries and launcher script.<br>\n",
    "Now, IntelliJ IDEA will open and ready to use. However, you can notice that still, we are running it from the Terminal. Closing IntelliJ IDEA will let you close the Terminal safely.<br>\n",
    "\n",
    "Now search for 'IntelliJ IDEA' in the dashboard and open it.<br>\n",
    "A common problem encountered on IntelliJ IDEA is its default keyboard shortcuts assigned to the Windows environment. This may cause unexpected behaviors of keyboard shortcuts on Linux environment. To fix this problem, open IntelliJ IDEA, go to Settings → Keymap and select \"Default for GNOME\" in the Keymap dropdown list.<br>\n",
    "\n",
    "### Polynote\n",
    "\n",
    "https://towardsdatascience.com/getting-started-with-polynote-netflixs-data-science-notebooks-47fa01eae156<br>\n",
    "`pip3 install jep jedi pyspark virtualenv`<br>\n",
    "Download latest release from https://github.com/polynote/polynote/releases and extract<br>\n",
    "`tar -zxvpf polynote-dist.tar.gz`<br>\n",
    "`cd polynote`<br>\n",
    "to open polynote simple run polynote.py<br>\n",
    "`./polynote.py`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Zeppelin notebooks\n",
    "\n",
    "__NOTE__<br>\n",
    "Only currently compatible with spark 2.4.4. If want to keep spark 3.0 as well then extract spark 2.4.4 into /usr/bin/spark and set this as SPARK_HOME in zeppelin-env.sh. Spark 3.0 will still be in /usr/local/spark/ and will be used for spark-shell etc as this is the path set in .bashrc.\n",
    "\n",
    "#### Setup for local zeppelin (without docker)\n",
    "\n",
    "__IMPORTANT: Zeppelin only works with java-8 so make sure that’s the version you have installed)__<br>\n",
    "\n",
    "##### Zeppelin Downloads\n",
    "http://zeppelin.apache.org/docs/0.8.1/quickstart/install.html#requirements\n",
    "Zeppelin Installation notes\n",
    "https://gist.github.com/pratos/b2e2937106980a867d0558cba46241b1\n",
    "Zeppelin Installation video\n",
    "https://www.youtube.com/watch?v=9cZldDfG9s0\n",
    "\n",
    "\n",
    "##### Setup from binary file\n",
    "https://zeppelin.apache.org/download.html<br>\n",
    "Download binary file zeppelin-0.8.1-bin-all.tgz from link above. <br>\n",
    "Then untar<br>\n",
    "`Tar -`<br>\n",
    "\n",
    "Set necessary environment variables below in .bashrc (NOTE JAVA_HOME should point to the source file not the binary) (Hadoop files necessary for read/write from s3)<br>\n",
    "`export SCALA_HOME=/usr/bin/scala`\n",
    "`export JAVA_HOME= /usr/lib/jvm/java-8-openjdk-amd64`\n",
    "`export SPARK_HOME=/usr/local/spark` (needs to be local to give access without sudo)<br>\n",
    "`export HADOOP_HOME=~/FILE_PATH/training/spark_scala/software/hadoop`<br>\n",
    "`export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:~/FILE_PATH/training/spark_scala/software/hadoop/share/hadoop/common/lib`<br>\n",
    "`export HADOOP_CONF_DIR=~/FILE_PATH/training/spark_scala/software/hadoop/etc/hadoop`<br>\n",
    "`export HADOOP_MAPRED_HOME=~/FILE_PATH/training/spark_scala/software/hadoop`<br>\n",
    "`export HADOOP_COMMON_HOME=~/FILE_PATH/training/spark_scala/software/hadoop`<br>\n",
    "`export HADOOP_HDFS_HOME=~/FILE_PATH/training/spark_scala/software/hadoop`<br>\n",
    "`export YARN_HOME=~/FILE_PATH/training/spark_scala/software/hadoop`<br>\n",
    "`export PATH=$PATH:~/FILE_PATH/training/spark_scala/software/hadoop/bin`<br>\n",
    "\n",
    "\n",
    "Update zeppelin-env.sh file by copying template and changing as appropriate<br>\n",
    "`\\cp zeppelin-env.sh.template zeppelin-env.sh`<br>\n",
    "then in zeppelin-env.sh insert following lines:<br>\n",
    "`export JAVA_HOME= /usr/lib/jvm/java-8-openjdk-amd64`<br>\n",
    "`export SPARK_HOME=/usr/bin/spark`<br>\n",
    "\n",
    "\n",
    "Change the port by modifying the zeppelin-site.xml, I usually set it to 8870 see below<br>\n",
    "First copy template<br>\n",
    "`cp zeppelin-env.sh.template zeppelin-env.sh`\n",
    "\n",
    "Then change port as below<br>\n",
    "`<property>`\n",
    "`  <name>zeppelin.server.port</name>`<br>\n",
    "`  <value>8070</value>`<br>\n",
    "`  <description>Server port.</description>`<br>\n",
    "`</property>`<br>\n",
    "\n",
    "Run command below (must be sudo)<br>\n",
    "`sudo zeppelin-0.8.1-bin-all/bin/zeppelin-daemon.sh start`<br>\n",
    "\n",
    "to stop simply use:<br>\n",
    "`sudo zeppelin-0.8.1-bin-all/bin/zeppelin-daemon.sh stop`<br>\n",
    "\n",
    "To check if port active:<br>\n",
    "`sudo lsof -i -P`\n",
    "\n",
    "\n",
    "##### Setup Zeppelin docker server from Scalable Data Science Course\n",
    "Prerequisites<br> \n",
    "\n",
    "(1) Docker/Docker-compose<br>\n",
    "Install <br>\n",
    "`Pip install docker-compose`<br>\n",
    "`Pip install docker`<br>\n",
    "`Sudo apt install docker.io`<br>\n",
    "\n",
    "`sudo pip uninstall docker docker-compose`<br>\n",
    "`sudo apt-get update`<br>\n",
    "`sudo apt-get upgrade`<br>\n",
    "`sudo apt-get install docker docker-compose`<br>\n",
    "\n",
    "Set permissions<br>\n",
    "`sudo chmod +x /usr/local/bin/docker-compose`<br>\n",
    "`sudo usermod -aG docker USER`<br>\n",
    "(Need to restart for group add to take effect)<br>\n",
    "\n",
    "\n",
    "hosts file<br>\n",
    "Hosts file in /etc/hosts should have following lines<br>\n",
    "\n",
    "`127.0.0.1       localhost`<br>\n",
    "`127.0.1.1       USERNAME_FOR_VM-VirtualBox`<br>\n",
    "\n",
    "\n",
    "The following lines are desirable for IPv6 capable hosts<br>\n",
    "`::1     ip6-localhost ip6-loopback`<br>\n",
    "`fe00::0 ip6-localnet`<br>\n",
    "`ff00::0 ip6-mcastprefix`<br>\n",
    "`ff02::1 ip6-allnodes`<br>\n",
    "`ff02::2 ip6-allrouters`<br>\n",
    "\n",
    "(2) Haskell<br>\n",
    "Follow instructions on https://docs.haskellstack.org/en/stable/install_and_upgrade/#linux<br>\n",
    "Run the command below in terminal to install Haskell<br>\n",
    "`wget -qO- https://get.haskellstack.org/ | sh`<br>\n",
    "\n",
    "(3) Pinot<br>\n",
    "Git clone link below and follow instructions<br>\n",
    "`git clone https://gitlab.com/tilo.wiklund/pinot`<br>\n",
    "Run the two commands below inside the pinot folder to install pinot<br>\n",
    "`Stack setup --allow-different-user`<br>\n",
    "`Stack build –allow-different-user`<br>\n",
    "\n",
    "\n",
    "##### Zeppelin notebooks run on EMR Cluster<br>\n",
    "Push your json format notebooks saved in folder zepArchives to your git repo<br>\n",
    "\n",
    "Log in to EMR cluster<br>\n",
    "ssh to server<br>\n",
    "`ssh  USERNAME@ec2_AWS`<br>\n",
    "or<br>\n",
    "`ssh USERNAME@IP_ADDRESS`<br>\n",
    "\n",
    "git clone your repo into the cluster<br>\n",
    "\n",
    "To convert notebooks and place in default folder first run python script zimport.py on directory of notebooks\n",
    "`python zimport.py notebook_directory`<br>\n",
    "This will place notebooks in default folder `/var/lib/zeppelin/notebook/`<br>\n",
    "(To check default folder `vim zeppelin-env.sh` in folder `/usr/lib/zeppelin/conf/` and check default location of zeppelin notebooks (variable `ZEPPELIN_NOTEBOOK_DIR`) its usually set to `/var/lib/zeppelin/notebook/`)<br>\n",
    "\n",
    "Open a tunnel in a separate terminal<br>\n",
    "`ssh -N -L HOST USER_CREDENTIAL`<br>\n",
    "\n",
    "open browser with url http://localhost:8890<br>\n",
    "Any notebooks you converted using zimport should now be visible on the zeppelin server where you can now run them on the EMR cluster.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Zeppelin notebooks for local Docker development<br>\n",
    "Setup of Docker containers (from SDS)<br>\n",
    "\n",
    "Minimal SDS docker<br>\n",
    "Has Zeppelin and hadoop<br>\n",
    "\n",
    "\n",
    "__OPTION 1: saving zeppelin files outside docker (Best option)__<br>\n",
    "Run line below in terminal specifying path where you want to store zeppelin notebook files.<br>\n",
    "\n",
    "docker-compose run -v FULL_PATH_TO_FOLDER_TO SAVE_ZEP_NOTEBOOKS:/root/zeppelin-0.8.0-bin-all/notebook -p 8080:8080 zeppelin<br>\n",
    "\n",
    "\n",
    "\n",
    "__OPTION 2: saving zeppelin files inside docker__<br> \n",
    "(note they will be lost if not saved outside docker before you shut down docker)<br>\n",
    "Start docker and zeppelin service<br>\n",
    "Go into docker-compose folder inside sds and start up docker. The below command will setup a docker with minimal services required to create Zeppelin notebooks.<br>\n",
    "docker-compose -f docker-compose-hadoop-zeppelin.yml up -d<br>\n",
    "\n",
    "Start the zeppelin service in the docker using the below command<br>\n",
    "docker-compose exec zeppelin bash<br>\n",
    "\n",
    "The dockerCompose/programs folder is loaded into this docker so any files you want available inside the docker should be placed in here before entering the docker. Currently inside the programs folder is my minimal sds repo.<br>\n",
    "\n",
    "Next run the zimport file on chosen directory of json notebook files (You will have converted dbc files to json using instructions below first)<br>\n",
    "Python zimport.sh DIRECTORY_NAME<br>\n",
    "\n",
    "For example to convert a folder holding all the json files for the dsd-2-x files run:<br>\n",
    "python programs/git/sds/babel/zimport.py programs/git/sds/zepArchives/sds-2-x<br>\n",
    "\n",
    "These notebooks should now appear at http://localhost:8080/<br>\n",
    "\n",
    "These files are saved locally in the docker at ~/zeppelin-0.8.0-bin-all/notebook<br>\n",
    "(NOTE: they will be deleted when docker closed unless you move them to the programs file where they will be persisted.)<br>\n",
    "\n",
    "\n",
    "\n",
    "#### Convert between databricks, JSON and Zeppelin notebook files using Pinot\n",
    "\n",
    "##### Convert databricks files to json files<br>\n",
    "https://github.com/lamastex/scalable-data-science/blob/master/_sds/basics/infrastructure/onpremise/dockerCompose/readmes/dbToZp.md<br>\n",
    "\n",
    "\n",
    "(1) Setup directory and clone necessary files<br>\n",
    "Setup git folder (usually in your home directory)<br>\n",
    "git clone my sds repo into this directory<br>\n",
    "`git clone https://github.com/mmcgov/sds.git`<br>\n",
    "(This is a minimal repo with all you need to babel between databricks and Zeppelin)<br>\n",
    "In the cloned sds directory you have the following folders<br>\n",
    "\n",
    "Babel – Contains scripts to convert dbc archive files to zeppelin notebook files<br>\n",
    "dbcArchives – folder containing dbcArchive files from Razz’s notes<br>\n",
    "zepArchives – Contains json versions of dbc Archive files<br>\n",
    "dockerCompose – docker container with zeppelin service<br>\n",
    "LICENSE  <br>\n",
    "README.md  <br>\n",
    "\n",
    "##### Convert dbcArchive files to json<br>\n",
    "In the babel folder vim into makeZeppelinnotes.sh file and change where necessary for your filepaths as below with line numbers for reference.<br>\n",
    "\n",
    "Set PINOT_DIR to be directory where you have your pinot folder from earlier<br>\n",
    "` 20 PINOT_DIR=~/FILE_PATH/training/pinot`<br>\n",
    "\n",
    "Set sds_DIR to be directory where you cloned my minimal repo sds<br>\n",
    " `22 sds_DIR=~/git/sds`<br>\n",
    "\n",
    "Set zp_GIT_DIR to be directory where zeppelin files will be stored, set to default where they are stored in sds folder so you shouldn’t have to change this.<br>\n",
    " `23 zp_GIT_DIR=$sds_DIR/zepArchives`<br>\n",
    "\n",
    "\n",
    "Set dbcArchive to be the dbc archive file from Razz’s you want to convert eg sds-2-x-dl<br>\n",
    " `32 dbcArchive=sds-2-x-dl`<br>\n",
    "\n",
    "\n",
    "\n",
    "This line should be fine and follow on from all the previous set variables<br>\n",
    "`39 stack exec pinot --allow-different-user   -- --from databricks --to zeppelin<br> $sds_DIR/dbcArchives/$dbcArchive.dbc -o $zp_GIT_DIR`<br>\n",
    "\n",
    "This line should be fine and follow on from all the previous set variables<br>\n",
    "`50 stack exec --allow-different-user    -- adder -f zeppelin -c $sds_DIR/babel/zeppelinInjectionCodes.txt  $(ls $zp_GIT_DIR/sds-$dbcArchive/*.json) -o $zp_GIT_DIR/sds-$dbcArchive`<br>\n",
    "\n",
    "Once finished with editing file save and run as bash script<br>\n",
    "`bash makeZeppelin.sh`<br>\n",
    "\n",
    "Go into zepArchives folder and json versions of all the notebooks should be there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemath\n",
    "Note Sagemath uses java version 11 but remember need to point to 8 for Hadoop/spark etc <br>\n",
    "Useful links<br>\n",
    "http://www.sagemath.org/<br>\n",
    "https://www.youtube.com/watch?v=A59flmBEVzk<br>\n",
    "`sudo apt-get install sagemath`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "586px",
    "width": "474px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "650px",
    "left": "590px",
    "top": "180px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
